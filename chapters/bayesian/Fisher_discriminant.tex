\section*{Линейный дискриминант Фишера}

Линейный дискриминант Фишера в первоначальном значении — метод, определяющий расстояние между распределениями двух разных классов объектов или событий. Он может использоваться в задачах машинного обучения при статистическом (байесовском) подходе к решению задач классификации. 

Предположим, что обучающая выборка удовлетворяет помимо базовых гипотез байесовского классификатора также следующим гипотезам:
\begin{itemize}
    \item Классы распределены по нормальному закону.
    \item Матрицы ковариаций классов равны.
\end{itemize}

Такой случай соответствует наилучшему разделению классов по дискриминанту Фишера (в первоначальном значении). Тогда статистический подход приводит к линейному дискриминанту, и именно этот алгоритм классификации в настоящее время часто понимается под термином линейный дискриминант Фишера.

\subsection*{Введение}

При некоторых общих предположениях байесовский классификатор сводится к формуле:
\[ 
a(x) = \mathrm{arg}\max_{yin Y} \lambda_{y} P_y p_y(x), 
\]
где $Y$ — множество ответов (классов), $x$ принадлежит множеству объектов $X$, $P_y$ — априорная вероятность класса $y$, $p_y(x)$ — функция правдоподобия класса $y$, $\lambda_{y}$ — весовой коэффициент (цена ошибки на объекте класса $y$).

При выдвижении всех указанных выше гипотез, кроме гипотезы о равенстве матриц ковариаций, данная формула принимает вид:
\[
a(x) = \mathrm{arg}\max_{yin Y} \left( ln(\lambda_{y} P_y) - \frac{1}{2}(x - \mu_y)^T \Sigma^{-1}_{y} (x - \mu_y) - \frac{1}{2}ln(\det{\Sigma^{-1}_{y}}) - \frac{n}{2}ln(2\pi) \right),
\]
где 
\[
\mu_y = \frac{1}{l_y} \sum^{l}_{\stackrel{i=1}{y_i = y}}x_i, \quad 
\Sigma_y = \frac{1}{l_y} \sum^{l}_{\stackrel{i=1}{y_i = y}}(x_i - \mu_y)(x_i - \mu_y)^T
\]
— приближения вектора математического ожидания и матрицы ковариации класса $y$, полученные как оценки максимума правдоподобия, $l$ — длина обучающей выборки, $l_y$ — количество объектов класса $y$ в обучающей выборке, $x \in \mathbb{R}^n$.

Данный алгоритм классификации является квадратичным дискриминантом. Он имеет ряд недостатков, одним из самых существенных из которых является плохая обусловленность или вырожденность матрицы ковариаций $\Sigma_y$ при малом количестве обучающих элементов класса $y$, вследствие чего при обращении данной матрицы $\Sigma^{-1}_{y}$ может получиться сильно искаженный результат, и весь алгоритм классификации окажется неустойчивым, будет работать плохо (возможна также ситуация, при которой обратная матрица $\Sigma^{-1}_{y}$ вообще не будет существовать). Линейный дискриминант Фишера решает данную проблему.

\textbf{Задача 1.} Каковы преимущества и недостатки использования квадратичного дискриминантного анализа (QDA) по сравнению с линейным дискриминантным анализом (LDA) в задачах классификации?


\subsection*{Основная идея алгоритма}

При принятии гипотезы о равенстве между собой ковариационных матриц алгоритм классификации принимает вид:
\[
a(x) = \mathrm{arg}\max_{yin Y} \left( ln(\lambda_{y} P_y) - \frac{1}{2}\mu_{y}^{T} \Sigma^{-1} \mu_y + x^T \Sigma^{-1} \mu_y \right),
\]
или 
\[
a(x) = \mathrm{arg}\max_{y\in Y} (\beta_y + x^T\alpha_y).
\]

Простота классификации линейным дискриминантом Фишера — одно из достоинств алгоритма: в случае с двумя классами в двумерном признаковом пространстве разделяющей поверхностью будет прямая. Если классов больше двух, то разделяющая поверхность будет кусочно-линейной. Но главным преимуществом алгоритма по сравнению с квадратичным дискриминантом является уменьшение эффекта плохой обусловленности ковариационной матрицы при недостаточных данных.

При малых $l_y$ приближения 
\[
\Sigma_y = \frac{1}{l_y} \sum^{l}_{\stackrel{i=1}{y_i = y}}(x_i - \mu_y)(x_i - \mu_y)^T
\]
дадут плохой результат, поэтому даже в тех задачах, где заведомо известно, что классы имеют различные формы, иногда бывает выгодно воспользоваться эвристикой дискриминанта Фишера и считать матрицы ковариаций всех классов одинаковыми. Это позволит вычислить некоторую "среднюю" матрицу ковариаций, используя всю выборку:
\[
\Sigma = \frac{1}{l} \sum^{l}_{i=1}(x_i - \mu_{y_i})(x_i - \mu_{y_i})^T,
\]
использование которой в большинстве случаев сделает алгоритм классификации более устойчивым.

\textbf{Задача 2.} Каковы основные предпосылки и ограничения линейного дискриминанта Фишера, и в каких случаях его применение может быть предпочтительнее по сравнению с квадратичным дискриминантом?

\subsection*{Выводы}

Эвристика линейного дискриминанта Фишера является в некотором роде упрощением квадратичного дискриминанта. Она используется с целью получить более устойчивый алгоритм классификации. Наиболее целесообразно пользоваться линейным дискриминантом Фишера, когда данных для обучения недостаточно. Вследствие основной гипотезы, на которой базируется алгоритм, наиболее удачно им решаются простые задачи классификации, в которых по формам классы "похожи" друг на друга.

Процесс классификации линейным дискриминантом Фишера можно описать следующей схемой:
\begin{enumerate}
    \item Обучение
    \begin{itemize}
        \item Оценивание математических ожиданий $\mu_y$
        \item Вычисление общей ковариационной матрицы $\Sigma$ и ее обращение
    \end{itemize}
    
    \item Классификация
    \begin{itemize}
        \item Использование формулы 
        \[
        a(x) = \mathrm{arg}\max_{yin Y} \left( ln(\lambda_{y} P_y) - \frac{1}{2}\mu_{y}^{T} \Sigma^{-1} \mu_y + x^T \Sigma^{-1} \mu_y \right)
        \]
    \end{itemize}
\end{enumerate}

\textbf{Задача 3.}Даны два класса объектов, представленные следующими данными:

\begin{itemize}
    \item Класс 1: $X_1 = {(2, 3), (3, 3), (2, 4)}$
    \item Класс 2: $X_2 = {(5, 6), (6, 5), (5, 7)}$
\end{itemize}

Найдите линейный дискриминант Фишера
\newline

\textit{Ответ к задаче 1}
\begin{itemize}
    \item QDA лучше подходит для задач, где классы имеют разные дисперсии и формы, и когда доступно достаточно данных для надежной оценки параметров.

    \item LDA может быть предпочтительнее в случаях с ограниченным количеством данных или когда классы можно считать линейно разделимыми.
\end{itemize}

\textit{Ответ к задаче 2}
Основные предпосылки линейного дискриминанта Фишера:
\begin{itemize}
    \item Нормальность: Предполагается, что данные в каждом классе распределены нормально.

    \item Однородность дисперсий: Линейный дискриминант предполагает одинаковые матрицы ковариаций для всех классов.

    \item Линейная разделимость: Предполагается, что классы можно разделить линейной границей.
\end{itemize}
Ограничения:
\begin{itemize}
    \item Если данные не удовлетворяют предпосылкам нормальности или однородности дисперсий, производительность линейного дискриминанта может значительно ухудшиться.

    \item Линейный дискриминант не может захватить сложные нелинейные зависимости между классами.
\end{itemize}
Когда предпочтительнее:
\begin{itemize}
    \item Линейный дискриминант может быть предпочтительнее квадратичного в случаях, когда:

    \item Данные имеют высокую размерность и при этом имеют достаточно малое количество образцов (линейный подход менее подвержен переобучению).

    \item Классы действительно линейно разделимы или близки к этому.
\end{itemize}
\textit{Пример:} В задачах распознавания лиц с использованием признаков (например, цветовые компоненты пикселей) линейный дискриминант может быть эффективным из-за высокой размерности данных и необходимости в простоте модели.

\textit{Ответ к задаче 3}
\begin{enumerate}
    \item Найдите средние векторы для каждого класса.
    
    Средние векторы:
    \[
    \mu_1 = \left(\frac{2+3+2}{3}, \frac{3+3+4}{3}\right) = \left(2.33, 3.33\right)
    \]
    
    \[
    \mu_2 = \left(\frac{5+6+5}{3}, \frac{6+5+7}{3}\right) = \left(5.33, 6.00\right)
    \]
    
    \item Вычислите матрицы дисперсии для каждого класса.
    
    Для класса 1:
    \[
    S_1 = \frac{1}{n_1-1} \sum_{i=1}^{n_1} (x_i - \mu_1)(x_i - \mu_1)^T
    \]
    
    После вычислений получаем:
    \[
    S_1 = \begin{pmatrix}
        0.33 & 0.33 \
        0.33 & 0.67
    \end{pmatrix}
    \]

    Для класса 2:
    \[
    S_2 = \frac{1}{n_2-1} \sum_{i=1}^{n_2} (x_i - \mu_2)(x_i - \mu_2)^T
    \]
    
    После вычислений получаем:
    \[
    S_2 = \begin{pmatrix}
        0.67 & -0.33 \
        -0.33 & 0.67
    \end{pmatrix}
    \]

    \item Найдите линейный дискриминант Фишера.
    
    Сначала находим объединённую матрицу дисперсии:
    \[
    S_W = S_1 + S_2 =
    \begin{pmatrix}
        0.33 + 0.67 & 0.33 - 0.33 \
        0.33 - 0.33 & 0.67 + 0.67
    \end{pmatrix}
    =
    \begin{pmatrix}
        1 & 0 \
        0 & 1.34
    \end{pmatrix}
    \]

    Теперь находим весовой вектор $w$:
    \[
    w = S_W^{-1}(\mu_1 - \mu_2) =
    \begin{pmatrix}
        1 & 0 \
        0 & 1.34
    \end{pmatrix}^{-1}
    \begin{pmatrix}
        2.33 - 5.33 \
        3.33 - 6
    \end{pmatrix}
    =
    \begin{pmatrix}
        -3 \
        -2.67
    \end{pmatrix}
    \]
\end{enumerate}