\section*{Качество классификации: Precision, Recall}

В задаче классификации \textbf{precision} (точность) и \textbf{recall} (полнота) являются ключевыми метриками для оценки качества предсказания, особенно в задачах с несбалансированными классами.

\subsection*{Определения}

Рассмотрим бинарную классификацию, то есть объект может быть либо положительным (positive), либо отрицательным (negative), и определим:

\begin{itemize}
    \item $TP$ (\textit{True Positives}) — количество объектов, правильно классифицированных как положительные.
    \item $FP$ (\textit{False Positives}) — количество объектов, ошибочно классифицированных как положительные.
    \item $FN$ (\textit{False Negatives}) — количество объектов, ошибочно классифицированных как отрицательные.
    \item $TN$ (\textit{True Negatives}) — количество объектов, правильно классифицированных как отрицательные.
\end{itemize}

На основе этих величин вычисляются:

\begin{enumerate}
    \item \textbf{Precision:}
    \[
    \text{Precision} = \frac{TP}{TP + FP}.
    \]
    Precision показывает долю истинно положительных объектов среди всех объектов, классифицированных как положительные.

    \item \textbf{Recall:}
    \[
    \text{Recall} = \frac{TP}{TP + FN}.
    \]
    Recall показывает долю истинно положительных объектов среди всех реально положительных объектов.
\end{enumerate}

\begin{figure}[h!!!!!!!!!!]
	\centering
	\includegraphics[width=0.65\linewidth]{chapters/model_selection/images/Precisionrecall.png}
\end{figure}

\subsection*{Интуитивное объяснение}

\begin{itemize}
    \item \textbf{Precision}: Насколько «точен» алгоритм, когда он говорит, что объект положительный? Если precision высокое, значит, ложные срабатывания ($FP$) минимальны.
    \item \textbf{Recall}: Насколько хорошо алгоритм находит все положительные объекты? Если recall высокое, значит, пропущенные положительные объекты ($FN$) минимальны.
\end{itemize}

\subsection*{Пример}

Классический пример использования метрик precision и recall - задача поиска спама на почте. В этом случае спам - положительная категория. Пусть у нас есть 100 писем, из которых 40 писем — спам, 60 писем — не спам.

Алгоритм классифицировал 50 писем как спам, из которых 30 писем действительно оказались спамом, а остальные 20 писем были ошибочно классифицированы как спам. Вычислим в этом случае precision и recall:

\begin{itemize}
    \item \textbf{Precision:}
    \[
    \text{Precision} = \frac{TP}{TP + FP} = \frac{30}{30 + 20} = 0.6.
    \]

    \item \textbf{Recall:}
    \[
    \text{Recall} = \frac{TP}{TP + FN} = \frac{30}{30 + 10} = 0.75.
    \]
\end{itemize}

\subsection*{Баланс между precision и recall}

Модель для каждого объекта на входе генерирует какое-то число на выходе. В простейшем варианте объект классифицируется как положительный, если это число больше некого выставленного порога, и как отрицательный в обратном случае. Увеличивая порог классификации, мы снижает количество False Positive объектов, потому precision увеличивается. При этом количество "незамеченных" моделью положительных объектов тоже вырастет, поэтому recall снизится. При уменьшении порога будет наблюдаться обратный эффект. Обычно стараются добиться компромиссного значения, при котором precision и recall оба принимают удовлетворительные значения. В некоторых случаях одна из метрик важнее другой:

\begin{itemize}
    \item Детекция спам-рассылок. В этом случае мы чаще всего не хотим пометить важные письма, как спам. Поэтому нужно снизить False Positive - важнее precision.
    \item Первичного выявление заболевания. Мы не хотим пропустить пациентов, которые на самом деле больны, только потому что модель сказала обратное. Поэтому важно снизить False Negative - в этом случае важнее recall. 
\end{itemize}

\subsection*{Интуитивное объяснение}

Если при получении положительного ответа от модели мы предпринимаем какое-либо действие, то precision важнее, когда действие обходится дорого, а recall важнее, когда бездействие обходится дорого. В примерах выше: помещение важного письма в папку "спам" (действие) может привести к финансовым потерям, а пропуск реального спама во "входящие" (бездействие) лишь заставит человека сделать это вручную. С другой стороны, при ложноположительном диагнозе человек пройдёт дополнительные анализы (действие), а ложноотрицательный может стоить ему жизни (бездействие).

\begin{figure}[h!!!!!!!!!!]
	\centering
	\includegraphics[width=0.65\linewidth]{chapters/model_selection/images/Precisionrecallcurve.png}
\end{figure}

\bigskip
\bigskip

Для того, чтобы изобразить баланс между двумя метриками, строят precision-recall кривую. Точки на ней соответствуют разным значениям порога.

\subsection*{F-метрика (дополнительно)}

Часто для оценки общего качества модели используется метрика $F$-мера, которая является гармоническим средним между precision и recall с параметром $\beta$:
\[
F_{\beta} = (1 + \beta^2) \cdot \frac{\text{Precision} \cdot \text{Recall}}{\beta^2 \cdot \text{Precision} + \text{Recall}}.
\]
Из формулы понятно, что $\beta$ определяет, насколько recall важнее по сравнению с precision. Наиболее часто используется $F_1$-мера:
\[
F_1 = 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}}.
\]

\subsection*{Задачи}

\subsection*{Задача 1: Простейшая модель}

Как будет выглядеть precision-recall кривая у простейшей модели, которая для любого объекта делает positive предсказание с вероятностью $1-t$ (при $t = 1$ все предсказания отрицательные, при $t = 0.5$ - половина)? Как в этом случае и с использованием метрик precision и recall подбирать параметр $t$, чтобы добиться лучшей работы модели?

\textbf{Ответ:}

В координатах ($recall$, $precision$) - отрезок с концами в (0, $\alpha$) и (1, $\alpha$), где $\alpha$ - доля положительных объектов в выборке (горизонтальный отрезок). Объяснение состоит в том, что доля $TP$ равна $\alpha (1-t)$, доля $TP + FN$ равна $\alpha$, а доля $TP + FP$ равна $(1-t)$. Таким образом, значения precision и recall не зависят от параметра $t$, то есть его изменение не изменит качество модели в этих метриках.

\subsection*{Задача 2: Дисбаланс классов}

Какая из метрик -- precision или recall -- будет больше в случае сильного дисбаланса классов на тестовой выборке (рассмотреть оба случая), если известно, что модель обучалась на сбалансированном датасете?

\textbf{Ответ:}

Если положительных объектов значительно меньше, чем отрицательных, то recall будет больше. Это связано с тем, что сбалансированная модель в этом случае будет допускать много False Positive ошибок. При обратном дисбалансе precision будет больше из-за большого количества False Negative ошибок.

\subsection*{Задача 3: Мошеннические транзакции}

Финансовая компания использует алгоритм для выявления мошеннических транзакций. Из 10,000 проверенных транзакций 500 транзакций являются мошенническими, 9,500 транзакций являются легитимными.

Алгоритм определил 600 транзакций как мошеннические, из которых 400 действительно оказались мошенническими.

\begin{enumerate}
    \item Вычислите $F_1$-меру.
    \item Насколько может измениться $F_1$-мера, если алгоритм пометит ещё 50 транзакций как мошеннические, в зависимости от того, являются они на самом деле мошенническими или нет?
\end{enumerate}

\textbf{Решение:}
Найдём precision и recall.
\begin{itemize}
    \item
    \[
    \text{$precision$} = \frac{400}{400 + 200} = \frac{400}{600} \approx 0.667.
    \]
    \item
    \[
    \text{$recall$} = \frac{400}{400 + 100} = \frac{400}{500} = 0.8.
    \]
    \item
    \[
    \text{$F_1$-score} = 2\frac{precision\cdot recall}{precision + recall} = 2\frac{0.667\cdot 0.8}{0.667 + 0.8} \approx 0.722
    \]

\end{itemize}

Обозначим за $\alpha$ долю тех новых помеченных транзакций, которые на самом деле являются мошенническими ($0 \le \alpha \le 1$). Понятно, что $TP$ увеличится на $50\alpha$, $FN$ уменьшится на $50\alpha$, а $TP + FP$ станет равным 650. Посчитаем precision, recall, изменение $F_1$-score в общем случае:

\begin{itemize}
    \item
    \[
    \text{$precision$} = \frac{400 + 50\alpha}{650}.
    \]
    \item
    \[
    \text{$recall$} = \frac{400 + 50\alpha}{500}.
    \]
    \item
    \[
    \text{$F_1$-score} = \frac{16 + 2\alpha}{23} \approx 0.696 + 0.087\alpha
    \]
    \item
    \[
    \text{$\Delta F_1$-score} \approx 0.696 + 0.087\alpha - 0.722 = -0.026 + 0.087\alpha
    \]
\end{itemize}

Таким образом, $F_1$-мера уменьшится на -0.026 если все новые помеченные транзакции на самом деле легитимные, увеличится на 0.061, если они все на самом деле мошеннические. Заметим, что $F_1$-мера останется неизменной, если $\alpha = \frac{1}{3}$, то есть изначальная доля ложноположительных среди всех помеченных.

\section*{Критерии скользящего контроля}
Скользящий контроль — это один из основных методов оценки качества модели и подбора её гиперпараметров. Он заключается в разделении данных на тренировочные и тестовые наборы несколько раз с последующим усреднением метрик качества. Это позволяет снизить влияние случайных факторов, связанных с конкретным разбиением данных.

\subsection*{Основные стратегии скользящего контроля}
Существуют несколько стратегий скользящего контроля, каждая из которых подходит для различных типов задач и данных:
\begin{itemize}
    \item \textbf{K-fold Cross-Validation}: данные разбиваются на $K$ равных частей. На каждой итерации одна часть используется как тестовая, остальные $K-1$ частей — для обучения. В итоге получаем $K$ оценок качества, которые усредняются.
    \item \textbf{Leave-One-Out (LOO)}: частный случай K-fold, где $K$ равно числу объектов. На каждой итерации используется ровно один объект для тестирования. Это даёт точную, но вычислительно затратную оценку.
    \item \textbf{Stratified K-fold Cross-Validation}: модификация K-fold, которая сохраняет соотношение классов в каждом fold. Используется для данных с дисбалансом классов.
    \item \textbf{Time Series Cross-Validation}: для временных данных разбиение производится с учётом хронологического порядка, чтобы тестовые данные всегда следовали за тренировочными.
\end{itemize}

\subsection*{Преимущества и недостатки скользящего контроля}
\textbf{Преимущества:}
\begin{itemize}
    \item \textit{Надёжная оценка качества}: усреднение по нескольким разбиениям делает метрики более устойчивыми к случайным вариациям.
    \item \textit{Эффективное использование данных}: каждая часть данных используется как для обучения, так и для тестирования.
    \item \textit{Универсальность}: подходит для большинства задач, включая классификацию, регрессию и временные ряды.
\end{itemize}

\textbf{Недостатки:}
\begin{itemize}
    \item \textit{Высокая вычислительная сложность}: для больших датасетов выполнение нескольких итераций обучения может быть ресурсоёмким.
    \item \textit{Риск утечки данных}: неправильное разбиение (например, использование связанных данных в разных folds) может привести к завышенной оценке качества.
\end{itemize}

\subsection*{Теоретическое обоснование использования}
Основная цель скользящего контроля — оценка обобщающей способности модели, то есть её способности работать с данными, которые не использовались при обучении. В идеале модель должна демонстрировать одинаково высокое качество как на тренировочной, так и на тестовой выборке. Скользящий контроль позволяет:
\begin{itemize}
    \item Снизить вероятность переобучения, так как тестирование производится на каждом этапе.
    \item Оценить влияние гиперпараметров на качество модели, подбирая их на основе средних метрик.
    \item Найти оптимальный компромисс между сложностью модели и её качеством (выбор более простой модели, если добавление параметров не даёт значимого прироста метрик).
\end{itemize}

\subsection*{Задачи}

\subsection*{Задача 1: Обоснование использования K-fold cross-validation}
Рассмотрим задачу классификации с $N=1000$ объектами. Модель обучается на $80\%$ данных и тестируется на оставшихся $20\%$. Какой метод даст более стабильную оценку качества: использование одной разбиения или 5-fold cross-validation? Почему?

\textbf{Ответ:}
Оценка качества модели при 5-fold cross-validation будет более стабильной, так как она основывается на усреднении результатов пяти итераций, где каждый объект хотя бы раз используется в тестовой выборке. Одно разбиение может дать случайную оценку, зависящую от конкретного разбиения.

\subsection*{Задача 2: Проблема дисбаланса классов в cross-validation}
В наборе данных 90\% объектов принадлежат классу $A$ и 10\% — классу $B$. Какой метод разбиения данных (обычный K-fold или Stratified K-fold) лучше использовать? Почему?

\textbf{Ответ:}
Лучше использовать Stratified K-fold cross-validation, так как он сохраняет пропорцию классов в каждой части выборки. Обычный K-fold может случайно распределить слишком много объектов одного класса в тестовую выборку, что сделает оценку качества модели необъективной.

\subsection*{Задача 3: Оптимизация гиперпараметра с помощью K-fold cross-validation}
Вы обучаете модель с одним гиперпараметром $\lambda$, который принимает значения из множества $\{0.1, 0.5, 1.0, 5.0\}$. Для оценки качества модели используется 4-fold cross-validation, и в каждой итерации вычисляется метрика $Accuracy$. Результаты по folds для каждого значения $\lambda$ следующие:

\begin{tabular}{|c|c|c|c|c|}
\hline
$\lambda$ & Fold 1 & Fold 2 & Fold 3 & Fold 4 \\ \hline
0.1       & 0.85   & 0.83   & 0.80   & 0.82   \\ \hline
0.5       & 0.88   & 0.87   & 0.85   & 0.86   \\ \hline
1.0       & 0.90   & 0.91   & 0.89   & 0.90   \\ \hline
5.0       & 0.78   & 0.75   & 0.80   & 0.76   \\ \hline
\end{tabular}

Какое значение $\lambda$ следует выбрать для модели? Найдите среднее значение $Accuracy$ для каждого $\lambda$ и выберите оптимальный гиперпараметр.

\textbf{Решение:}
Для каждого значения $\lambda$ найдём среднее значение метрики $Accuracy$:
\begin{itemize}
    \item Для $\lambda = 0.1$:
    \[
    \text{Mean Accuracy} = \frac{0.85 + 0.83 + 0.80 + 0.82}{4} = \frac{3.3}{4} = 0.825.
    \]
    \item Для $\lambda = 0.5$:
    \[
    \text{Mean Accuracy} = \frac{0.88 + 0.87 + 0.85 + 0.86}{4} = \frac{3.46}{4} = 0.865.
    \]
    \item Для $\lambda = 1.0$:
    \[
    \text{Mean Accuracy} = \frac{0.90 + 0.91 + 0.89 + 0.90}{4} = \frac{3.6}{4} = 0.90.
    \]
    \item Для $\lambda = 5.0$:
    \[
    \text{Mean Accuracy} = \frac{0.78 + 0.75 + 0.80 + 0.76}{4} = \frac{3.09}{4} = 0.7725.
    \]
\end{itemize}

Оптимальным значением $\lambda$ является $1.0$, так как оно даёт максимальное среднее значение $Accuracy$:
\[
\lambda_{\text{opt}} = 1.0, \quad \text{Mean Accuracy} = 0.90.
\]

