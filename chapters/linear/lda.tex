\section*{Линейный дискриминантный анализ (LDA)}

Линейный дискриминантный анализ (LDA) — это статистический метод для решения задач классификации, который используется для поиска линейных комбинаций признаков, наиболее эффективно разделяющих два или более классов. Основной задачей LDA является минимизация внутриклассовой дисперсии и максимизация межклассовой дисперсии, что позволяет лучше различать классы на основе их характеристик.

\subsection*{Предположения LDA}

LDA предполагает следующие условия для классов данных:
\begin{enumerate}
    \item \textbf{Нормальность распределений.} Признаки \( x \in \mathbb{R}^m \) для каждого класса \( C_k \) распределены нормально с параметрами: средним вектором \( \mu_k \in \mathbb{R}^m \) и ковариационной матрицей \( \Sigma_k \in \mathbb{R}^{m \times m} \).
    \item \textbf{Одинаковые ковариационные матрицы.} Все классы имеют одинаковую ковариационную матрицу \( \Sigma \), что значительно упрощает задачу классификации, так как для построения линейной границы между классами используется одна и та же матрица ковариаций.
\end{enumerate}

Согласно этим предположениям, распределение признаков в каждом классе можно выразить через многомерное нормальное распределение:
\[
P(x | C_k) = \frac{1}{(2\pi)^{m/2} |\Sigma|^{1/2}} \exp\left( -\frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k) \right),
\]
где \( \mu_k \) — средний вектор признаков для класса \( C_k \), а \( \Sigma \) — ковариационная матрица, одинаковая для всех классов.

\subsection*{Задача LDA}

Цель LDA заключается в нахождении линейного классификатора, который разделяет классы с минимальной ошибкой. Для этого LDA ищет линейную функцию от признаков \( x \) вида:
\[
\delta(x) = w^T x + b,
\]
где \( w \) — вектор весов, \( b \) — смещение. Классы разделяются гиперплоскостью, заданной уравнением:
\[
w^T x + b = 0.
\]

Классификация основывается на выборе того класса, для которого значение \( \delta(x) \) наибольшее:
\[
\hat{y} = \text{sign}(w^T x + b).
\]

\subsection*{Вывод классификатора LDA}

Для решения задачи классификации методом LDA необходимо максимизировать правдоподобие для наблюдаемых данных, предполагая, что каждый класс имеет нормальное распределение с одинаковыми ковариационными матрицами. Рассмотрим два класса \( C_1 \) и \( C_2 \). Обозначим средние векторы классов как \( \mu_1 \) и \( \mu_2 \), а ковариационную матрицу как \( \Sigma \).

В соответствии с теоремой Байеса, для каждого класса вероятность \( P(C_k | x) \) вычисляется как:
\[
P(C_k | x) = \frac{P(x | C_k) P(C_k)}{P(x)}.
\]
Для того чтобы классифицировать объект \( x \), необходимо выбрать класс с наибольшей апостериорной вероятностью. Так как \( P(x) \) не зависит от класса, задача сводится к сравнению правдоподобий:
\[
P(C_1 | x) > P(C_2 | x) \quad \text{или} \quad \log P(C_1 | x) > \log P(C_2 | x).
\]

Подставив выражения для \( P(x | C_1) \) и \( P(x | C_2) \), получаем:
\[
- \frac{1}{2} (x - \mu_1)^T \Sigma^{-1} (x - \mu_1) + \log P(C_1) > - \frac{1}{2} (x - \mu_2)^T \Sigma^{-1} (x - \mu_2) + \log P(C_2).
\]

Упростив это выражение, мы приходим к линейному решению:
\[
w^T x + b = 0,
\]
где
\[
w = \Sigma^{-1} (\mu_1 - \mu_2), \quad b = -\frac{1}{2} (\mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2) + \log \frac{P(C_1)}{P(C_2)}.
\]

Таким образом, линейное правило классификации LDA основывается на разности средних \( \mu_1 \) и \( \mu_2 \), взвешенных инвертированной ковариационной матрицей \( \Sigma^{-1} \).

\subsection*{Оптимизация LDA}

Задача оптимизации LDA заключается в том, чтобы найти параметры классификатора, минимизируя ошибку классификации. Это достигается путём минимизации внутриклассовой дисперсии и максимизации межклассовой дисперсии. Внутриклассовая дисперсия характеризует разброс объектов внутри одного класса, а межклассовая дисперсия — разброс между классами. В результате, LDA позволяет найти оптимальную гиперплоскость, которая максимизирует различие между классами.

\subsection*{Основные шаги алгоритма LDA}
\begin{enumerate}
    \item Рассчитываем среднее для каждого класса \( \mu_k \) и ковариационную матрицу для всего набора данных \( \Sigma \).
    \item Вычисляем вектор весов \( w = \Sigma^{-1} (\mu_1 - \mu_2) \) и смещение \( b = -\frac{1}{2} (\mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2) + \log \frac{P(C_1)}{P(C_2)} \).
    \item Классифицируем объект \( x \), вычисляя \( w^T x + b \) и присваивая метку класса, соответствующую наибольшему значению.
\end{enumerate}

\section*{Задача 1: Основное правило классификации}

Дано два класса данных \( C_1 \) и \( C_2 \), каждый из которых представлен наборами векторов признаков \( X_1, X_2 \in \mathbb{R}^m \). Пусть векторы признаков для каждого класса распределены нормально с одинаковыми ковариационными матрицами: \( \Sigma_1 = \Sigma_2 = \Sigma \in \mathbb{R}^{m \times m} \) и средними \( \mu_1 \) и \( \mu_2 \).

\begin{enumerate}
    \item Используя принцип максимизации правдоподобия, выведите линейное правило классификации в LDA для двух классов \( C_1 \) и \( C_2 \).
    \item Докажите, что это правило эквивалентно выбору гиперплоскости, которая разделяет два класса, используя линейную комбинацию признаков. Определите, как вычисляется граница между классами.
\end{enumerate}

\subsection*{Решение:}

1. Для нахождения линейного классификатора мы предполагаем, что признаки \( x \) для каждого класса следуют нормальному распределению с различными средними \( \mu_1, \mu_2 \), но одинаковыми ковариационными матрицами \( \Sigma \). Согласно методу максимизации правдоподобия, логарифм правдоподобия для каждого класса выглядит следующим образом:

   \[
   \log P(C_k | x) = -\frac{1}{2} \log |\Sigma| - \frac{1}{2} (x - \mu_k)^T \Sigma^{-1} (x - \mu_k) + \log P(C_k)
   \]

   Для классификации выбираем класс с максимальным правдоподобием, что эквивалентно выбору гиперплоскости, которая разделяет два класса. После упрощений получаем линейное правило классификации:

   \[
   \delta(x) = w^T x + b \quad \text{где} \quad w = \Sigma^{-1} (\mu_1 - \mu_2), \quad b = -\frac{1}{2} (\mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2) + \log \frac{P(C_1)}{P(C_2)}
   \]

   Гиперплоскость, разделяющая классы, определяется по линейному выражению \( w^T x + b = 0 \).

2. Линейное правило классификации \( \delta(x) \) позволяет разделить классы с помощью гиперплоскости, где весовой вектор \( w \) пропорционален разности средних \( \mu_1 - \mu_2 \), а смещение \( b \) зависит от ковариационной матрицы и вероятностей классов. Это утверждение доказывается тем, что линейный классификатор LDA минимизирует ошибку классификации для нормальных распределений с одинаковыми ковариациями.

\section*{Задача 2: Оптимизация классификатора}

Дано два класса данных \( C_1 \) и \( C_2 \), каждый из которых имеет нормальное распределение признаков с одинаковыми ковариационными матрицами \( \Sigma \), но с различными средними значениями \( \mu_1 \) и \( \mu_2 \).

\begin{enumerate}
    \item Используя предположения о нормальности распределений и одинаковости ковариационных матриц, выведите общее правило для классификатора LDA для разделения классов \( C_1 \) и \( C_2 \).
    \item Покажите, что гиперплоскость, разделяющая классы, определяется разностью средних \( \mu_1 - \mu_2 \) и инвертированной ковариационной матрицей \( \Sigma^{-1} \). Докажите, что правило классификации LDA можно записать как линейную функцию от признаков.
\end{enumerate}

\subsection*{Решение:}

1. Используя предположения о нормальности распределений и одинаковости ковариационных матриц, максимизируем правдоподобие:

   \[
   P(x | C_1) = \frac{1}{(2\pi)^{m/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu_1)^T \Sigma^{-1} (x - \mu_1)\right)
   \]
   \[
   P(x | C_2) = \frac{1}{(2\pi)^{m/2} |\Sigma|^{1/2}} \exp\left(-\frac{1}{2} (x - \mu_2)^T \Sigma^{-1} (x - \mu_2)\right)
   \]

   Для классификации, принимаем решение на основе сравнения логарифмов правдоподобий:

   \[
   \log P(C_1 | x) - \log P(C_2 | x)
   \]

   Упрощая выражения, получаем линейное правило классификации:

   \[
   w^T x + b = 0 \quad \text{где} \quad w = \Sigma^{-1} (\mu_1 - \mu_2), \quad b = -\frac{1}{2} (\mu_1^T \Sigma^{-1} \mu_1 - \mu_2^T \Sigma^{-1} \mu_2) + \log \frac{P(C_1)}{P(C_2)}
   \]

2. Гиперплоскость, разделяющая два класса, имеет уравнение \( w^T x + b = 0 \), где \( w \) пропорционален разности средних \( \mu_1 - \mu_2 \), а \( b \) зависит от ковариационной матрицы и вероятностей классов. Это доказательство основано на том, что для нормальных распределений с одинаковыми ковариационными матрицами оптимальное решение для классификации представляет собой линейную функцию от признаков.

\section*{Задача 3: Оценка ошибки классификации}

Предположим, что у нас есть линейный классификатор, полученный методом LDA, который разделяет два класса \( C_1 \) и \( C_2 \). Пусть обучающий набор данных состоит из \( n \) объектов: \( \{(x_i, y_i)\} \), где \( x_i \in \mathbb{R}^m \), а \( y_i \in \{-1, 1\} \) — метки классов. Классификатор работает по правилу: если \( w^T x + b \geq 0 \), то класс \( C_1 \), иначе класс \( C_2 \).

\begin{enumerate}
    \item Докажите, что ошибка классификации на обучающих данных для классификатора, построенного методом LDA, может быть выражена как сумма индикаторов неверной классификации для каждого примера.
    \item Для случая, когда классы разделены линейной гиперплоскостью, выведите верхнюю границу ошибки классификации с использованием теоремы о обобщающей способности линейных классификаторов.
\end{enumerate}

\subsection*{Решение:}

1. Ошибка классификации на обучающих данных для классификатора \( h(x) = \text{sign}(w^T x + b) \) выражается как сумма индикаторов неверной классификации:

   \[
   \text{Ошибка} = \frac{1}{n} \sum_{i=1}^{n} \mathbb{I}(y_i \neq \text{sign}(w^T x_i + b))
   \]

   где \( \mathbb{I}(\cdot) \) — индикатор неверной классификации.

2. Для оценки ошибки на тестовых данных используется теорема о обобщающей способности, которая дает верхнюю границу ошибки классификации через максимальное расстояние от гиперплоскости до ближайших точек обучающей выборки. Для линейных классификаторов верхняя граница ошибки может быть выражена как:

   \[
   \text{Ошибка} \leq \frac{1}{\sqrt{n}} \cdot \left( \max_{i} \| x_i \| \right)
   \]

   Эта граница зависит от структуры обучающей выборки и обеспечивает оценку ошибки классификатора на новых данных.
   