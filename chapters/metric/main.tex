\section*{Часто используемые ядра \(K(r)\)}

\begin{figure}[h]
    \centering
    \includegraphics[width=\textwidth]{chapters/metric/images/I1.png}
    \caption{Графики ядер}
    \label{fig:kernels}
\end{figure}

\begin{align*}
\Pi(r) &= [{\lvert r \rvert \leq 1}] \quad \text{— прямоугольное} \\
T(r) &= (1 - \lvert r \rvert) [{\lvert r \rvert \leq 1}] \quad \text{— треугольное} \\
E(r) &= (1 - r^2) [{\lvert r \rvert \leq 1}] \quad \text{— квадратичное (Епанечникова)} \\
Q(r) &= (1 - r^2)^2 [{\lvert r \rvert \leq 1}] \quad \text{— квартическое} \\
G(r) &= \exp(-2r^2) \quad \text{— гауссовское}
\end{align*}


\section*{Выбор ядра \(K\) и ширины окна \(h\)}

\noindent
\(h \in \{\textcolor{red}{0.1}, 1.0, \textcolor{blue}{3.0}\}\), гауссовское ядро \(K(r) = \exp(-2r^2)\).
Графики с различной шириной окна \(h\):
\begin{align*}
    \centering
    \includegraphics[width=\textwidth]{chapters/metric/images/I2.png}
    \label{fig:kernel_choice}
\end{align*}

\begin{itemize}
    \item Гауссовское ядро \(\Rightarrow\) гладкая аппроксимация
    \item Ширина окна существенно влияет на точность аппроксимации
\end{itemize}

\section*{Выбор ядра \(K\) и ширины окна \(h\)}

\noindent
\(h \in \{\textcolor{red}{0.1}, 1.0, \textcolor{blue}{3.0}\}\), треугольное ядро \(K(r) = (1 - \lvert r \rvert) [{\lvert r \rvert \leq 1}]\). Графики с разными значениями \(h\) при треугольном ядре:
\begin{align*}
    \centering
    \includegraphics[width=\textwidth]{chapters/metric/images/I3.png}
    \label{fig:kernel_triangle}
\end{align*}

\begin{itemize}
    \item Треугольное ядро \(\Rightarrow\) кусочно-линейная аппроксимация
    \item Аппроксимация не определена, если в окне нет точек выборки
\end{itemize}

\section*{Выбор ядра \(K\) и ширины окна \(h\)}

\begin{itemize}
    \item \textbf{Ядро \(K(r)\)}
    \begin{itemize}
        \item существенно влияет на гладкость функции \( a_h(x) \),
        \item слабо влияет на качество аппроксимации.
    \end{itemize}
    \item \textbf{Ширина окна \(h\)}
    \begin{itemize}
        \item существенно влияет на качество аппроксимации.
    \end{itemize}
    \item \textbf{Переменная ширина окна по \(k\) ближайшим соседям:}
    \[
    w_i(x) = K\left( \frac{\rho(x, x_i)}{h(x)} \right), \quad h(x) = \rho(x, x^{(k+1)})
    \]
    где \(x^{(k)}\) — \(k\)-й сосед объекта \(x\).

    \item \textbf{Оптимизация ширины окна по скользящему контролю:}
    \[
    \text{LOO}(h, X^\ell) = \sum_{i=1}^\ell \left( a_h(x_i; X^\ell \setminus \{x_i\}) - y_i \right)^2 \to \min_h
    \]
\end{itemize}

\section*{Проблема выбросов (эксперимент на синтетических данных)}

\noindent
\(\ell = 100\), \(h = 1.0\), гауссовское ядро \(K(r) = \exp(-2r^2)\)

\vspace{0.5em}

{\color{red}Две из 100 точек — выбросы с ординатами \(y_i = 40\) и \(-40\)}

\vspace{0.5em}

{\color{blue}Синяя кривая — выбросов нет}

\begin{align*}
    \centering
    \includegraphics[width=\textwidth]{chapters/metric/images/I4.png}
    \label{fig:kernel_triangle}
\end{align*}

\section*{Проблема выбросов и локально взвешенное сглаживание}

\textbf{Проблема выбросов:} точки с большими случайными ошибками \(y_i\) сильно искажают функцию \(a_h(x)\)

\vspace{1em}
\textbf{Основная идея:} \\
чем больше величина ошибки \(\varepsilon_i = \lvert a_h(x_i; X^\ell \setminus \{x_i\}) - y_i \rvert\), \\
тем больше прецедент \((x_i, y_i)\) похож на выброс, \\
тем меньше должен быть его вес \(w_i(x)\).

\vspace{1em}
\textbf{Эвристика:} \\
домножить веса \(w_i(x)\) на коэффициенты \(\gamma_i = \tilde{K}(\varepsilon_i)\), \\
где \(\tilde{K}\) — ещё одно ядро, вообще говоря, отличное от \(K(r)\).

\vspace{1em}
\textbf{Рекомендация:} \\
квартическое ядро \(\tilde{K}(\varepsilon) = K_Q \left( \frac{\varepsilon}{6 \, \mathrm{med}\{\varepsilon_i\}} \right)\), \\
где \(\mathrm{med}\{\varepsilon_i\}\) — медиана вариационного ряда ошибок.

\section*{Алгоритм LOWESS (LOcally WEighted Scatter plot Smoothing)}

\vspace{1em}
\textcolor{blue}{\textbf{Вход:}} \(X^\ell\) — обучающая выборка; \\
\textcolor{blue}{\textbf{Выход:}} коэффициенты \(\gamma_i, \quad i = 1, \ldots, \ell\);

\vspace{1em}
инициализация: \(\gamma_i := 1, \quad i = 1, \ldots, \ell\);

\vspace{1em}
\textcolor{blue}{\textbf{повторять}}
\begin{itemize}
    \item оценки скользящего контроля в каждом объекте:
    \[
    a_i := a_h(x_i; X^\ell \setminus \{x_i\}) = \frac{\sum\limits_{j=1, j \neq i}^{\ell} y_j \gamma_j K\left( \frac{\rho(x_i, x_j)}{h(x_i)} \right)}{\sum\limits_{j=1, j \neq i}^{\ell} \gamma_j K\left( \frac{\rho(x_i, x_j)}{h(x_i)} \right)}, \quad i = 1, \ldots, \ell;
    \]
    \item \(\gamma_i := \tilde{K}(\lvert a_i - y_i \rvert), \quad i = 1, \ldots, \ell;\)
\end{itemize}

\textcolor{blue}{\textbf{пока}} коэффициенты \(\gamma_i\) не стабилизируются;

\section*{Пример работы LOWESS на синтетических данных}

\noindent
\(\ell = 100\), \(h = 1.0\), гауссовское ядро \(K(r) = \exp(-2r^2)\)

\vspace{1em}

Две из 100 точек — выбросы с ординатами \(y_i = 40\) и \(-40\)

\vspace{1em}

В данном случае LOWESS сходится за несколько итераций:
\begin{align*}
    \centering
    \includegraphics[width=\textwidth]{chapters/metric/images/I5.png}
    \label{fig:kernel_triangle}
\end{align*}

\section{Задачи}
\subsection{Задача 1}
Объясните, как выбор ядра \(K(r)\) влияет на гладкость регрессионной функции \(a_h(x)\). Приведите примеры различных ядер и их влияния.
\subsection{Ответ:}
Ядро \(K(r)\) определяет форму и вес окрестности точки, используемой для оценки регрессионной функции. Гладкие ядра, такие как гауссовское, дают более плавные оценки, в то время как менее гладкие, такие как прямоугольное, приводят к менее сглаженным функциям. Например:

- Гауссовское ядро \(K(r) = \exp(-r^2)\) — даёт плавные, гладкие оценки.

- Треугольное ядро \(K(r) = 1 - \lvert r \rvert\) (при \(\lvert r \rvert \leq 1\)) — более резкое, но все ещё гладкое.

- Прямоугольное ядро \(K(r) = 0.5\) (при \(\lvert r \rvert \leq 1\)) — приводит к кусочно-постоянной функции.

\subsection{Задача 2}
Приведите пример оптимального веса \(w_i(x)\) в алгоритме LOWESS, если известно распределение ошибок в данных. Поясните, как это распределение должно влиять на выбор веса, и почему весовое ядро \(\tilde{K}\) должно учитывать распределение ошибок.

\subsection{Ответ:}
Если ошибка \(\varepsilon_i\) распределена согласно некоторому известному распределению, например нормальному, \( \varepsilon_i \sim \mathcal{N}(0, \sigma^2)\), то весовой коэффициент должен минимизировать дисперсию предсказания. Весовая функция \(\tilde{K}(\varepsilon_i)\) должна убывать, когда \(\varepsilon_i\) отклоняется от некоторого центрального значения (0 для нормального распределения), чтобы уменьшить влияние выбросов:

\[
w_i(x) = \exp\left(-\frac{\varepsilon_i^2}{2\sigma^2}\right)
\]

Этот вес сильнее подавляет ошибки, отклоняющиеся от средней, что уменьшает их влияние на итоговую модель. Выбор весового ядра \(\tilde{K}\) должен учитывать распределение ошибок, чтобы учесть типичные вариации данных.

\subsection{Задача 3}
На основе метода LOWESS предложите способ оценки доверительного интервала для предсказаний. Выведите формулу для доверительного интервала и объясните, как она может быть использована для оценки надежности модели.

\subsection{Ответ:}

\quad 1. Построение модели:

   - Применим LOWESS для построения основной модели, получив предсказанные значения \( \hat{y}_i \) для каждого \(x_i\).

2. Оценка остаточной дисперсии:

   - Вычислим остаточные отклонения \(e_i = y_i - \hat{y}_i\).
   
   - Оценим дисперсию ошибок \(\sigma^2\) как среднеквадратическое отклонение:

     \[
     \sigma^2 = \frac{1}{n-k} \sum_{i=1}^{n} e_i^2
     \]

   где \(n\) — число точек данных, \(k\) — число параметров (в случае LOWESS, это скорее степень полинома в локальных регрессиях).

3. Построение доверительного интервала:

   - Для каждого предсказанного значения \( \hat{y}_i \) построим доверительный интервал, используя стандартное отклонение остаточных ошибок и критические значения из t-распределения:

     \[
     \hat{y}_i \pm t_{\alpha/2, n-k} \cdot \sqrt{\frac{\sigma^2}{n_i}}
     \]

   где \( t_{\alpha/2, n-k} \) — квантиль t-распределения с уровнем значимости \(\alpha\), и \(n_i\) — эффективное число точек в окрестности \(x_i\) (окрестность, которая использовалась для регрессии, может быть выражена размером окна или числом соседей).

4. Использование доверительных интервалов:

   - Доверительные интервалы позволяют пользователю оценить, насколько "надёжны" предсказанные значения. Узкие интервалы свидетельствуют о высокой уверенности.
   
   - Визуализация доверительных интервалов на графиках помогает выявлять области, где модель может быть неопределённой или подверженной ошибкам.
