\section*{Экспоненциальное семейство распределений}
Всегда ранее мы предполагали наша модель $f(\cdot, \alpha)$ предсказывает мир с какой-то ошибкой, т.е.
$$
y_i = f(x_i, \alpha) + \varepsilon_i,
$$
где $\varepsilon_i \sim \mathcal{N}(0, \sigma_i^2)$. Тоесть ошибка распределена нормально. Однако в общем случае это может быть не так и ошибка может быть приходить из какого-то другого распределения, что пораждает другое распределение и у самих $y_i$. Долее будем считать, что распределение ошибок приходит из экспоненциального семейства.

\noindent\textbf{Определение.} Распределение $\mathsf{P}$ принадлежит \textit{экспоненциальному семейству} ($\mathsf{Exp}(\theta_i, \phi_i)$) с параметрами $\theta_i, \phi_i$ и параметрами-функциями $c(\theta), h(y, \phi)$, если его пролтность представляется в следующем виде:
$$
p(y_i) = \exp\left(
  \frac{y_i\theta_i - c(\theta_i)}{\phi_i} + h(y_i, \phi_i)
\right)
$$.
Предполагается, что $c(\theta)$ - непрерывно дважды дифференцируема, $c'(\theta)$ - монотонна, а $h(y, \phi)$ - непрерывна по $y$.

Пусть $y_i \sim \mathsf{Exp}(\theta_i, \phi_i)$, тогда
\begin{align*}
  \mu_i := \mathsf{E}y_i &= c'(\theta_i)\\
  \mathsf{D}y_i &= \frac{c''(\theta_i)}{\phi_i}
\end{align*}

\noindent\textbf{Определение.} \textit{Функцией связи} называется функция обратная $c'(\theta_i)$.
$$
g(\mu_i) = (c')^{-1}(\mu_i) \text{ - функция связи.}
$$

Несмотря на то, что распределений из экспоненциального семейства мало - на практике выходит так, что почти всякое распределение - экспоненциально.

\section*{Принцип максимального правдоподобия для GLM}
В случае, если нам известно, что величины $y_i$ не из нормального распределения, но из экспоненциального - пользоваться методом наименьших квадратов нельзя. Нужно честно выписывать принцип максимального правдоподобия.
Пусть $y_i \sim \mathsf{Exp}(\theta_i, \phi_i)$. Выпишем функцию правдоподобия.
$$
L(\alpha) = \ln \prod_{i=1}^l p(y_i \mid \theta_i, \phi_i) = \sum_{i=1}^l \frac{y_i \theta_i - c(\theta_i))}{\phi_i} + h(y_i, \phi_i) \to \max_\alpha.
$$
Если дополнительно известно, что выборка гомоскедастична (тоесть все $\phi_i$ равны), то слагаемое $h(y_i, \phi_i)$ - можно убрать, ак как $h$ не зависет ни от объектов, ни от параметров модели.

Из постановки GLM мы знаем, что $\theta_i = x_i^\top\alpha$.

Будем находить максимум правдоподобия методом Ньютона-Рафсона
$$
\alpha^{t+1} = \alpha^t + h_t \left(L''(\alpha^t)\right)^{-1}L'(\alpha^t).
$$
Явно выпишем эти матрицы:
$$
\frac{\partial L(\alpha)}{\partial \alpha_j} = \sum_{i=1}^l \frac{y_i - c'(x_i^\top\alpha)}{\phi_i}f_j(x_i)
$$
$$
\frac{\partial^2 L(\alpha)}{\partial \alpha_j\partial \alpha_k} = \sum_{i=1}^l \frac{c''(x_i^\top\alpha)}{\phi_i}f_j(x_i)f_k(x_i)
$$
Можно заметить, что $L''(\alpha)$ можно представить, как произведение трех матриц - одной диагональной, и двух матриц "объект-признак".
$$
L''(\alpha) = F^\top D_t F,
$$
где $F$ - матрица "объект-признак", а $D_t$ - диагональна, $[D_t]_{i,i} = \frac{c''(x_i^\top\alpha^t)}{\phi_i}$.
Матрицу $D_t$ можно разделить на две и каждую из частей присоеденить к $F$:
\begin{align*}
  D_t &= W_t W_t & W_t &= \sqrt{D_t}\\
  L''(\alpha) &= F^\top D_t F = \widetilde{F}^\top\widetilde{F} & \widetilde{F} &= W_t F.
\end{align*}.
Теперь преобразуем вектор $L'(\alpha)$. Можно его представить как произведение $\widetilde{F}$ на другой вектор $\widetilde{y}^t$.
\begin{align*}
  L'(\alpha) &= \widetilde{F}\widetilde{y}^t & \widetilde{y}^t_i &= \frac{y_i - c'(x_i^\top\alpha^t)}{\sqrt{\phi_i c''(x_i^\top\alpha^t)}}
\end{align*}
Этот вектор $\widetilde{y}^t$ можно воспринимать как отнормированный вектор ответов. Если вспонить, что $\mathsf{E}(y_i) = c'(\theta_i)$, $\mathsf{D}(y_i) = c''(\theta_i)$, а $\theta_i = x^\top \alpha$, то получается, что $\widetilde{y}^t$ - как раз стандатная нормировка $y$ в предположении, что он взят из распределения $\mathsf{Exp}(x^\top \alpha^t, \phi_i)$.

Тогда итерацию в методе Ньютона-Рафсона можно записать так
$$
\alpha^{t+1} = \alpha^t + h_t \left(L''(\alpha^t)\right)^{-1}L'(\alpha^t) =
\alpha^t + h_t \left(\widetilde{F}^\top\widetilde{F}\right)^{-1}\widetilde{F}\widetilde{y}^t
$$
Но тогда мы получили новую задачу регрессии
$$
Q(\alpha) = \|\widetilde{F} \alpha - \widetilde{y}^t\|^2 \to \min_{\alpha}.
$$
$\alpha^{t+1}$ является решением задачи выше. Таким образом, иттеративно подбирая $\alpha^t$ метод сойдется к некоторому значению $\alpha$, которое и будет являться решением регрессии методом максимального правдоподобия.

\section*{Логистическая регрессия как частный случай GLM}
Рассмотрим задачу регрессии. Сделаем всего 2 предположения:
\begin{enumerate}
  \item $y_i$ - бернулевские случайные величины. Тоесть у $y_i$ есть вероятность быть в том или ином классе.
  \item Модель линейна и $\mu_i = \mathsf{E}y_i$ монотонно зависет от $\theta_i = x_i^\top\alpha$.
\end{enumerate}
Лишь из этих двух предположений мы сразу получаем функцию связи
$$
\theta_i = \gamma(\mu_i) = \ln\frac{\mu_i}{1-\mu_i}
$$
$$
\mu_i = c'(\theta_i) = \frac{1}{1+\exp(\theta_i)} = \sigma(-\theta_i) \text{ - сигмойда}.
$$
Записывая же метод максимального правдоподобия, получаем минимизацию критерия negative-log-loss.
$$
-L(\alpha) = -\sum_{i=1}^l \ln p(y_i \mid x_i) = \sum_{i=1}^l -y_i \ln \mu_i + (y_i - 1) \ln (1 - \mu_i) \to \min_\alpha
$$
Распишем случаи для $y_i$
$$
\ln p(y_i \mid x_i) = 
\begin{cases}
  \ln\mu_i = \ln(\sigma(\theta_i)) = \ln(\sigma(x_i^\top\alpha)) & y_i = 1\\
  \ln(1 - \mu_i) = \ln(\sigma(-\theta_i)) = \ln(\sigma(-x_i^\top\alpha)) & y_i = 0
\end{cases} = \ln(\sigma(\widetilde{y}_ix_i^\top\alpha)),
$$
где $\widetilde{y}_i = 2y_i - 1$. Тогда принцип максимального правдоподобия можно перепиcать как
$$
-L(\alpha) = \sum_{i=1}^l -\ln(\sigma(\widetilde{y}_ix_i^\top\alpha)) = 
\sum_{i=1}^l \ln(1 + \exp(-\widetilde{y}_ix_i^\top\alpha)).
$$

\section*{Задачи}

\subsection*{Задача 1}

Доказать, что если $y_i \sim \mathsf{Exp}(\theta_i, \phi_i)$, существует матожидание и дисперсия $y_i$ и выполены условия на $c(\theta)$, $h(y, \phi)$, то
\begin{align*}
  \mathsf{E}y_i &= c'(\theta_i)\\
  \mathsf{D}y_i &= \frac{c''(\theta_i)}{\phi_i}
\end{align*}

\begin{proof}
  Рассмотрим функцию
  $$
  f(y_i, \theta) = \exp\left(
  \frac{y_i\theta - c(\theta)}{\phi_i} + h(y_i, \phi_i)\right).
  $$
  Тогда $p(y_i) = f(y_i, \theta_i) \geq 0$. Из условий на $c(\theta)$ и $h(y_i, \phi_i)$ получаем, что $f(y_i, \theta)$ непрерывна вместе со своей частной производной $\frac{\partial f}{\partial \theta}(y_i, \theta)$. Хотим продифференцировать интеграл от $f$, для этого рассмотрим прямоугольник $[-n, n]\times[\alpha, \beta] \ni (y_i, \theta_i)$. Тогда верно, что
  $$
  \frac{\partial}{\partial\theta} \int_{-n}^n f(y_i, \theta)\,dy_i = \int_{-n}^n\frac{\partial f}{\partial\theta}(y_i,\theta)\,dy_i.
  $$
  
  Разберемся сначала с правой частью.
  \begin{multline*}
    \int_{-n}^n\frac{\partial f}{\partial\theta}(y_i,\theta)\,dy_i =
    \int_{-n}^n\frac{y_i - c'(\theta)}{\phi_i} f(y_i, \theta)\,dy_i =\\=
    \frac{1}{\phi}
    \underbrace{
      \int_{-n}^n y_if(y_i, \theta)\,dy_i
    }_{\to\mathsf{E}_\theta y_i \text{ при } n\to\infty} +
    \frac{c'(\theta)}{\phi}
    \underbrace{
      \int_{-n}^n f(y_i, \theta)\,dy_i
    }_{\to 1 \text{ при } n\to\infty}\to
    \frac{\mathsf{E}_\theta y_i - c'(\theta)}{\phi_i} \text{ при } n\to\infty,
  \end{multline*}
  Причем не трудно видеть, что сходимость равномерная.
  
  Теперь разберемся с левой частью. Мы уже знаем, что
  $$
  \int_{-n}^n\frac{\partial f}{\partial\theta}(y_i,\theta)\,dy_i \rightrightarrows \frac{\mathsf{E}_\theta y_i - c'(\theta)}{\phi_i} \text{ при } n\to\infty,
  $$
  Но тогда можно делать предельный переход под знаком производной!
  $$
  \frac{\partial}{\partial\theta} \int_{-n}^n f(y_i, \theta)\,dy_i \to \frac{\partial}{\partial\theta} \int_{-\infty}^\infty f(y_i, \theta)\,dy_i = \frac{\partial}{\partial\theta} 1 = 0 \text{ при } n\to\infty.
  $$
  Приравнивая левую и правую части в точке $\theta_i$, получаем
  $$
  0 = \frac{\mathsf{E}y_i - c'(\theta_i)}{\phi_i},
  $$
  $$
  \mathsf{E}y_i = c'(\theta_i).
  $$
  Для того, чтобы разобраться с дисперсией нужно проделать все тоже самое, только с $\frac{\partial f}{\partial\theta}(y_i, \theta)$, вместо $f(y_i, \theta)$. Имеем
  $$
  \frac{\partial^2}{\partial\theta^2} \int_{-n}^n f(y_i, \theta)\,dy_i =
  \frac{\partial}{\partial\theta} \int_{-n}^n\frac{\partial f}{\partial\theta}(y_i,\theta)\,dy_i = \int_{-n}^n\frac{\partial^2 f}{\partial\theta^2}(y_i,\theta)\,dy_i.
  $$
  Вновь начнем с правой части и аналогично получим
  \begin{multline*}
    \int_{-n}^n\frac{\partial^2 f}{\partial\theta^2}(y_i,\theta)\,dy_i =
    \int_{-n}^n \frac{y_i^2 - 2y_ic'(\theta) + c'(\theta)^2 - c''(\theta)\phi_i}{\phi_i^2} f(y_i,\theta)\,dy_i \rightrightarrows\\
    \rightrightarrows
    \frac{1}{\phi_i^2}\mathsf{E}_\theta y_i^2 -
    \frac{2c'(\theta)}{\phi_i^2}\mathsf{E}_\theta y_i +
    \frac{c'(\theta)^2 - c''(\theta)\phi_i}{\phi_i^2} \text{ при } n\to\infty.
  \end{multline*}
  В левой части аналогично предел равен нулю. Приравнивая все в точке $\theta_i$ получаем
  $$
  0 = \frac{1}{\phi_i^2}\mathsf{E} y_i^2 -
  \frac{2c'(\theta_i)}{\phi_i^2}\mathsf{E} y_i +
  \frac{c'(\theta_i)^2 - c''(\theta_i)\phi_i}{\phi_i^2}
  $$
  Не забудем, что $c'(\theta_i) = \mathsf{E} y_i$.
  $$
  0 = \mathsf{E} y_i^2 - 2\left(\mathsf{E} y_i\right)^2 + \left(\mathsf{E} y_i\right)^2 - c''(\theta_i)\phi_i
  $$
  $$
  c''(\theta_i)\phi_i = \mathsf{E} y_i^2 - \left(\mathsf{E} y_i\right)^2 = \mathsf{D}y_i
  $$
\end{proof}

\subsection*{Задача 2}

Компания aperture science занимается разработкой и тестированием портальных устройств. Перед тестом находится подопытный человек, специальная комманда исследователей точно измеряют его характеристики такие как: рост, iq и объективная оценка привлекательности (от -7 до 100). Испытуемому выдают портальную пушку и запускают в комнату тестирования. Про нее тоже есть информация: общий объем комнаты, сложность испытания, количество камер и количество турелей. Отдельно обученная группа математиков подсчитывала количетсво открытых порталов во время первых 10 миинут тестирования. Этой величиной называют pq испытуемого (или portal quotient). Поскольку портальная пушка - крайне энергозатратное устройство - компания нанимает отдельную группу аналитиков для того, чтобы по уже имеющимся данным предсказывать pq испытуемого и не допускать до тестирования тех, чей pq окажется слишком большим.

Необходимо получить регрессионную модель с помощью принципа максимального правдоподобия и построить GLM, которые по данным $x_i$ будут предсказывать $pq_i$. Считать, что $pq_i \sim \text{Pois}(\lambda_i)$.

\noindent\textit{Решение.}
Пуассоновское распределение принадлежит экспоненциальному семейству, действительно
$$
p(pq_i \mid \lambda_i) = \frac{e^{-\lambda_i}\lambda_i^{pq_i}}{pq_i!} = \exp\left(
pq_i\ln(\lambda_i) - \lambda_i - \ln(pq_i!)
\right),
$$
Причем параметры равны
\begin{align*}
  \theta_i &= \ln(\lambda_i),\\
  c(\theta_i) &= \lambda_i = e^\theta_i,\\
  \phi_i &= 1.
\end{align*}
Функция связи при этом равна
\begin{align*}
  \mu_i &= c'(\theta_i) = e^\theta_i &
  \theta_i = g(\mu_i) = \ln(\mu_i)
\end{align*}

Воспользуемся методом максимального правдоподобия:
$$
-L(\alpha) = -\sum_{i=1}^l \ln p(pq_i \mid \mu_i) = -\sum_{i=1}^l pq_i \theta_i - \mu_i =
\sum_{i=1}^l \exp\left(x_i^\top \alpha\right) - pq_i \cdot x_i^\top \alpha \to \min_\alpha
$$
Тогда для решения можно просто обучить линейную регрессию с функцией потерь, описаной выше.

Посмотрим также как бы выглядела MSE-регрессия, полученая в методе Ньютона-Рафсона.
$$
[W_t]_{i,i} = \sqrt{\frac{c''(x_i^\top\alpha^t)}{\phi_i}} = \exp\left(\frac{x_i^\top\alpha^t}{2}\right)
$$
$$
[\widetilde{F}]_{i,j} = f_i(x_j)\exp\left(\frac{x_j^\top\alpha^t}{2}\right)
$$
$$
\widetilde{y}^t_i = \frac{y_i - c'(x_i^\top\alpha^t)}{\sqrt{\phi_i c''(x_i^\top\alpha^t)}} = 
y_i\exp\left(-\frac{x_i^\top\alpha^t}{2}\right) - \exp\left(\frac{x_i^\top\alpha^t}{2}\right)
$$

\subsection*{Задача 3}

К сожалению, далеко не всякий набор распределений принадлежит экспоненциальному семейству. Нужно доказать, что распределения
$U[0, \theta]$, где $\theta > 0$ не принадлежать экспоненциальному семейству.

\begin{proof}
  Воспользуемся неравеством Рао-Крамера. Тогда для всякой выборки из распределения принадлежащего экспоненциальному семейству верно неравество
  $$
  \mathsf{D}_\theta \widehat\theta(X) \geq \frac{\tau'(\theta)}{ni(\theta)},
  $$
  где $\widehat\theta(X)$ - несмещенная оценка $\tau(\theta)$, а $n$ - размер выборки.
  В частности
  $$\mathsf{D}_\theta \widehat\theta(X) \gg \frac{1}{n}.$$
  
  Однако для равномерного распределения можно выбрать $\widehat\theta(X) = \frac{n+1}{n} X_{(n)}$, как несмещенную оценку $\theta$. Дисперсия этой оценки равна
  $$
  \mathsf{D}_\theta \widehat\theta(X) = \frac{\theta^2}{n^2 + 2n} = o\left(\frac{1}{n^2}\right).
  $$
  Получили противоречие - полученая оценка имеет слишком мальекую дисперсию. Это значит, что $U[0, \theta]$ не принадлежит экспоненциальному семейству.
\end{proof}
