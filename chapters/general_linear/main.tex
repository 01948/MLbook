\section*{Регуляризация. Гребневая регрессия.}

Итак, мы решили задачу многомерной линейной регрессии методом наименьших квадратов. Казалось бы, что всё уже хорошо, однако в решении мы воспользовались крайне опасным инструментом -- умножением на обратную матрицу. При таком подходе можно запнуться об подводные камни. В этом разделе мы поймём суть проблемы и предложим эффективный способ с ней бороться.

\subsection*{Мультиколлинеарность и число обусловленности матрицы.}
Вспомним, как записывается решение МНК:
$$w^* = (F^T F)^{-1} F^T y, \; \|w^*\|^2 = \sum_{j=1}^n \frac{1}{\lambda_j}(v_j^T y)^2.$$
Видим проблему, если $\lambda_j$ маленькие, то решение <<взрывается>>. Это происходит, если найдется $U \in \mathbb{R}^n$ такое, что $Fu \approx 0$, то есть столбцы $F$ почти линейно зависимы. Формализуем понятие <<взрыва>> решения.

\noindent\textbf{Определение.} \textit{Числом обусловленности} 
положительно определённой квадратной матрицы $S$ называется величина
$$\mu(S) := \|S\| \cdot \|S^{-1}\|.$$

Теперь мы видим, что при умножении обратной матрицы на вектор, относительная погрешность может увеличиться в $\mu(S)$ раз:
$$\frac{\|\delta(S^{-1}u)\|}{\|S^{-1}u\|} = \frac{\|S^{-1} \delta u\|}{\|S^{-1}u\|} \leqslant \mu(S)\frac{\|\delta u\|}{\|u\|}.$$
Неравенство следует из того факта, что
$$\|S^{-1}\delta u\| \leqslant \|S^{-1}\| \cdot \|\delta u\| \;\; \text{и} \;\; \|u\| \leqslant \|S\| \cdot \|S^{-1}u\| \; \Rightarrow \; \|S^{-1}u\| \geqslant \frac{\|u\|}{\|S\|}.$$
Также здесь $\delta u$ обозначает разность между истинным значением и наблюдением.

Следовательно, положив $S := F^T F$, $u = F^T y$ и $w^* = (F^T F)^{-1} F^T y$ получим, что погрешности измерения ($\delta u$ выше) признаков $f_j(x_i)$ и ответов $y_i$ могут увеличиться в $\mu(F^T F)$ раз.

Таким образом, если матрица $F^TF$ плохо обусловлена (значение $\mu$ большое), то решение $w^*$ неустойчиво, то есть содержит большие по модулю координаты $w_j^*$. Это приводит к переобучению. Действительно, $\|Fw^* - y\|$ на обучении маленькое, а вот $\|F'w^* - y'\|$ на тесте большое, поскольку даже при небольшой вариации $F'$ по сравнению с $F$, умножение на <<большой>> вектор $w^*$ портит значения.

Чтобы побороть этот эффект применяют \textit{регуляризацию}, реализованную в \textit{гребневой регрессии}.

\subsection*{Гребневая регрессия (ridge regression).}
Метод гребневой регрессии прост, добавим штраф за увеличение $\ell_2$-нормы вектора весов $w$.
$$Q_\tau(w) = \|Fw - y\|^2 + \tau \|w\|^2 \to \min_w,$$
где $\tau$ -- неотрицательный параметр регуляризации.

Решается новая задача абсолютно также, продифференцируем по $w$:
\begin{gather*}
    \frac{\partial Q_\tau(w)}{\partial w} = 2 F^T (Fw - y) + 2\tau w = 0; \\
    w_\tau^* = (F^TF + \tau I_n)^{-1} F^Ty.
\end{gather*}
Добавку $\tau I_n$ иногда называют <<гребнем>>, оттого и <<гребневая регрессия>>.

\subsection*{Регуляризация.}
Теперь изучим подробнее, что такого нам даёт параметр регуляризации.
Давайте заметим, что мы всё ещё можем повторить предыдущее решение и применить сингулярное разложение.
\begin{gather*}
    F^TF + \tau I_n = UDV^TVDU^T + \tau I_n = UD^2U^T + \tau UI_nU^T = U(D^2 + \tau I_n)U^T; \\
    w_\tau^* = U(D^2 + \tau)^{-1}DV^Ty = \sum_{j=1}^n \frac{\sqrt{\lambda_j}}{\lambda_j + \tau}u_j(v_j^Ty); \\
    Fw_\tau^* = VDU^Tw_\tau^* = V\left[D(D^2 + \tau I_n)^{-1}D\right]V^Ty = \sum_{j=1}^n \frac{\lambda_j}{\lambda_j + \tau} v_j(v_j^Ty); \\
    \|w_\tau^*\|^2 = \|(D^2 + \tau)^{-1}DV^Ty\|^2 = y^TV\left[D(D^2 + \tau)^{-2}D\right]V^Ty = \sum_{j=1}^n \frac{\lambda_j}{(\lambda_j + \tau)^2}(v_j^Ty)^2.
\end{gather*}
Таким образом, мы устранили проблему маленьких $\lambda_j$ добавив в знаменателе $\tau$. Правда теперь $Fw^* \not= Fw_\tau^*$, зато решение стало гораздо устойчивее.

Теперь поговорим о том как выбирать параметр $\tau$. Сразу заметим, что сингулярное разложение позволяет нам не пересчитывать каждый раз $(F^TF + \tau I_n)^{-1}$, а вычислить единожды матрицы $U, D, V$, и в таком случае пересчёт $(D^2 + \tau I_n)^{-1}$ происходит за линию, так как матрицы диагональные.

Оставим часть данных под контрольную выборку, на которой будем подбирать параметр $\tau$. За счёт линейной скорости пересчёта можем реализовать эффективный поиск по сетке. Пусть $X^k = (x_i', y_i')_{i=1}^k$ -- контрольная выборка.
$$\underset{k \times n}{F'} = \begin{pmatrix}
    f_1(x_1') & \dots & f_n(x_1') \\
    \dots & \dots & \dots \\
    f_1(x_k') & \dots & f_n(x_k')
\end{pmatrix}, \quad \underset{k \times 1}{y'} = \begin{pmatrix}
    y_1' \\
    \dots \\
    y_k'
\end{pmatrix}.$$
Как долго происходит вычисление функционала $Q$ при условии, что в сетке $T$ параметров (то есть мы проверяем все $\tau$ на некотором отрезке с шагом $\frac{1}{T}$)?
$$Q(w_\tau^*, X^k) = \|F'w_\tau^* - y'\|^2 = \left\|\underset{k \times n}{\underbrace{F'U}} \underset{\text{диагональная} \; n \times n}{\underbrace{\text{diag}\left(\frac{\sqrt{\lambda_j}}{\lambda_j + \tau}\right)}}\underset{n \times 1}{\underbrace{V^Ty}} - y'\right\|^2.$$
Видим, что внутри достаточно один раз вычислить $F'U$ за $O(kn^2)$, один раз вычислить $V^Ty$ за $O(kn)$ и затем $T$ раз вычислить центральный элемент за $O(n)$ и умножить на соседей за $O(kn)$. Оставшиеся операции укладываются в $O(k)$.
$$\text{Число операций поиска по сетке:} \; O(kn^2 + knT).$$

\subsection*{Сжатие и сокращение <<эффективной размерности>>.}
Иногда эту регуляризацию называют \textit{сжатием} (shrinkage) или \textit{сокращением весов} (weight decay). Действительно, заметим, что
$$\|w_\tau^*\|^2 = \sum_{j=1}^n \frac{\lambda_j}{(\lambda_j + \tau)^2}(v_j^Ty)^2 < \sum_{j=1}^n \frac{1}{\lambda_j} (v_j^Ty)^2 = \|w^*\|^2,$$
то есть регуляризованный вектор весов строго меньше обычного.

Вместе с этим ещё говорят, что регуляризация сокращает <<эффективную размерность>>. Здесь подразумевается размерность подпространства, на которое мы проектируем с помощью оператора $F(F^TF)^{-1}F^T$. Несложное наблюдение: размерность подпространства при проекции равна следу матрицы оператора проекции.
$$\text{tr } F(F^TF)^{-1}F^T = \text{tr } (F^T F)^{-1} F^TF = \text{tr } I_n = n.$$
При использовании регуляризации:
$$\text{tr } F(F^TF + \tau I_n)^{-1}F^T = \text{tr } \text{diag}\left(\frac{\lambda_j}{\lambda_j + \tau}\right) = \sum_{j=1}^n \frac{\lambda_j}{\lambda_j + \tau} < n.$$
Выше мы пользовались свойством $\text{tr}(A_1 \dots A_m) = \text{tr}(A_m A_1 \dots A_{m-1})$.

\newpage
\subsection*{Задачи.}

\subsubsection*{Задача 1.} Как мы уже поняли, число обусловленности может быть полезной характеристикой. При этом мы говорили, что обычное решение МНК ломают маленькие собственные значение. Хочется связать эти понятия, поэтому предлагается доказать следующее утверждение для симметричной положительно определённой матрицы $S$ (это согласуется с $S := F^TF$):
$$\mu(S) = \frac{\lambda_{\max}(S)}{\lambda_{\min}(S)},$$
где $\lambda_{\max}(S)$ и $\lambda_{\min}(S)$ максимальное и минимальное собственное значение матрицы $S$ соответственно.

\noindent\textit{Решение.} Докажем сначала, что $\|S\| = \lambda_{\max}(S)$. Действительно, $S$ симметрична и положительно определена, значит существует базис, состоящий из собственных векторов матрицы $S$, обозначим их за $e_1, \dots, e_n$.
\begin{gather*}
\|S\| = \max_{u \not= 0} \frac{\|Su\|}{\|u\|} = \frac{\|\lambda_1c_1e_1 + \dots + \lambda_nc_ne_n\|}{\|c_1e_1 + \dots + c_ne_n\|} = \sqrt{\frac{\lambda_1^2c_1^2 + \dots + \lambda_n^2c_n^2}{c_1^2 + \dots + c_n^2}} \leqslant \\
\sqrt{\frac{\lambda_{\max}(S)^2(c_1^2 + \dots + c_n^2)}{c_1^2 + \dots + c_n^2}} = \lambda_{\max}(S).
\end{gather*}
Максимум достигается, если $u$ собственный вектор, соответствующий $\lambda_{\max}(S)$. Альтернативно распишем $S = C^{-1} \text{diag}(\lambda_i) C$, где $C$ -- матрица перехода к базису из собственных векторов, следовательно $S^{-1} = C^{-1} \text{diag}(1/\lambda_i) C$. Таким образом,
$$\mu(S) = \|S\| \cdot \|S^{-1}\| = \lambda_{\max}(S) \cdot \lambda_{\max}(S^{-1}) = \frac{\lambda_{\max}(S)}{\lambda_{\min}(S)}.$$
% Распишем по определению нормы матриц.
% $$\mu(S) = \|S\| \cdot \|S^{-1}\| = \max_{\|u\| = 1} \|Su\| \cdot \max_{\|u\| = 1} \|S^{-1}u\| =
% \frac{\max_{\|u\| = 1}\|Su\|}{\min_{\|v\| = 1} \|Sv\|} = \frac{\lambda_{\max}(S)}{\lambda_{\min}(S)}.$$
% Здесь мы воспользовались тем фактом, что $\displaystyle \max_{\|u\| = 1} \|S^{-1}u\| = \frac{1}{\min_{\|v\| = 1} \|Sv\|}$

\subsubsection*{Задача 2.} Заметим, что регуляризация может грубо действовать на дроби $\displaystyle\frac{\lambda_j}{\lambda_j + \tau}$, например мы точно знаем, что для дробей с достаточно большими собственными значениями в идеале надо было бы выбрать $\tau = 0$. Предложим такой способ, применим штраф покоординатно:
$$Q_\tau(w) = \|Fw - y\|^2 + \sum_{j=1}^n\tau_jw_j^2 \to \min_w.$$
Как тогда измениться решение $w_\tau^*$ и квадрат его нормы $\|w_\tau^*\|^2$?

\noindent\textit{Решение.} Это задача с подвохом. Увы, в жизни не всегда хорошо даже то, что диагонально. Казалось бы, продифференцировав всё выражение мы получим, что
$$w_\tau^* = (F^TF + \text{diag}(\tau_j))^{-1} F^Ty.$$
На этом этапе всё чисто. Однако когда мы попытаемся преобразовать это выражение через сингулярное разложение, то столкнёмся с проблемой.
$$F^TF + \text{diag}(\tau_j) = UDV^TVDU^T + \text{diag}(\tau_j) \overset{(!!!)}{=} UD^2U^T + U\text{diag}(\tau_j)U^T = \dots$$
Обратите внимание на переход с восклицательными знаками. В теории выше мы воспользовались переходом $U I_n U^T = I_n$ и для единичной матрицы он верен, но вот для произвольной диагональной, увы, уже нет.
$$\begin{pmatrix}
    \frac{1}{2} & -\frac{\sqrt{3}}{2} \\
    \frac{\sqrt{3}}{2} & \frac{1}{2}
\end{pmatrix} \cdot \begin{pmatrix}
    1 & 0 \\
    0 & 2
\end{pmatrix} \cdot \begin{pmatrix}
    \frac{1}{2} & \frac{\sqrt{3}}{2} \\
    -\frac{\sqrt{3}}{2} & \frac{1}{2}
\end{pmatrix} = \begin{pmatrix}
    \frac{7}{4} & -\frac{\sqrt{3}}{4} \\
    -\frac{\sqrt{3}}{4} & \frac{5}{4}
\end{pmatrix}.$$

\subsubsection*{Задача 3.} В древних скрижалях Самсууса, первого пекаря Хлебостана, скрыта тайна идеальной самсы. Команда археологов университета Зерноэматики смогла перевести почти весь рецепт, но вот беда, коэффициенты в одном из уравнений для выпечки стёрлись без шанса на восстановление. Сейчас его можно записать как $f(x) = \beta_1 + \beta_2 x + \beta_3 x^2 + \beta_4 \cos x$. К счастью команда нашла способ промоделировать поведение левой части (то есть $f(x)$) с некоторой точностью для всех значений $x \in [0, 1]$ с шагом $0.1$. Ваша задача построить модель, которая сможет предсказать остальные значения этой функции $f(x)$. Ответьте на вопрос, стоит ли применять регуляризацию. А если бы значения $f(x)$ приходили из другого множества?

\noindent\textit{Решение.} Сформулируем задачу на языке многомерной линейной регрессии. У нас есть $11$ объектов-чисел, числовые признаки которых это просто их значения на функциях $1, x, x^2, \cos x$. Тогда модель может быть составлена так:
$$F = \begin{pmatrix}
    1 & 1 & \dots & 1 \\
    0 & 0.1 & \dots & 1 \\
    0 & 0.01 & \dots & 1 \\
    \cos 0 & \cos 0.1 & \dots & \cos 1
\end{pmatrix}^T, \;\; w = \begin{pmatrix}
    \beta_1 \\
    \beta_2 \\
    \beta_3 \\
    \beta_4
\end{pmatrix}.$$
Главный вопрос, стоит ли применять регуляризацию? Конечно, вспомним, что $\cos x = 1 - \frac{x^2}{2} + o(x^3)$ в окрестности точки $0$, а значит у нас имеется почти линейная зависимость $1, 3$ и $4$ столбца (обратите внимание, что матрица транспонирована). Ну и теперь мы видим, когда регуляризация действительно нужна, а именно, когда значения $x$ достаточно малы (для $|x| < 1$ формула Тейлора аппроксимирует очень хорошо). Поскольку $|\cos(x)| \leqslant 1$, то уже для значений $|x| > 2$ разложение в ряд Тейлора выше ведёт себя плохо, и можно не применять регуляризацию. В любом случае, вы всегда сможете понять, что зависимость имелась, если значения $w^*$ окажутся очень большими.
