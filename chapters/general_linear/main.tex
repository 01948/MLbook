\section*{Lasso-регрессия (L1-регуляризация)}

Lasso-регрессия (Least Absolute Shrinkage and Selection Operator) включает $\ell_1$-регуляризацию, которая одновременно выполняет отбор признаков и предотвращает переобучение. Задача оптимизации формулируется следующим образом:
\[
\min_{\beta \in \mathbb{R}^p} \frac{1}{2n} \sum_{i=1}^n (y_i - x_i^T \beta)^2 + \lambda \|\beta\|_1,
\]
где $\|\beta\|_1 = \sum_{j=1}^p |\beta_j|$, а $\lambda > 0$ — параметр регуляризации. Штраф $\ell_1$ приводит к занулению некоторых коэффициентов $\beta_j$, тем самым автоматически выполняя отбор признаков.

\subsection*{Задача 1}
Как на основе Lasso-регрессии определить значимость признаков в линейной модели? 

\subsection*{Задача 2}
Сохраняет ли L1-регуляризация следующие свойства оптимизационной задачи:
\begin{itemize}
    \item гладкость,
    \item выпуклость,
    \item сильную выпуклость?
\end{itemize}

\subsection*{Задача 3}
Как известно, метод SVM сводится к решению задачи квадратичного программирования. Сохраняет ли эта задача квадратичность при добавлении L1-регуляризации? Если нет, какие методы оптимизации можно использовать для эффективного решения этой задачи?
