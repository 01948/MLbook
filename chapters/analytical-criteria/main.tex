\section{Аналитические внешние критерии}

\subsection{Введение}

\par Аналитические внешние критерии используются для отслеживания сложности используемой модели. Это необходимо для того, чтобы находить баланс между точностью модели на обучающей выборке и её обобщающей способностью.

\subsection{Критерии регуляризации}

\par \textbf{Регуляризатор} - аддитивная добавка к внутреннему критерию, обычно штраф за сложность (complexity penalty) модели $A$. 

\begin{align*}
    Q_{per} (\mu, X^l) = Q_{\mu}(X^l) + \text{штраф}(A)
\end{align*}


\par Для линейных моделей

1. Классификация $A = \{a(x) = \text{sign}\langle w, x \rangle\}$

2. Регрессия $A = \{a(x) = \langle w, x \rangle \}$

\par Существуют следующие виды регуляризации

\begin{enumerate}
    \item $L_2$-регуляризация (RIDGE)
        \begin{align*}
            \text{штраф}(A) =  \tau \|w\|_2^2 = \tau \sum\limits_{i = 1}^n w_i^2
        \end{align*}
        Может приводить к обнулению некоторых, несущественных признаков, что делает модель более разреженной
    \item $L_1$-регуляризация (LASSO)
        \begin{align*}
            \text{штраф}(A) = \tau \|w\|_1 = \tau \sum\limits_{i = 1}^n |w_i|
        \end{align*}
    \item $L_0$-регуляризация (AIC, BIC):
        \begin{align*}
            \text{штраф}(A) = \tau\|w\|_0 = \tau\sum\limits_{i = 1}^n [w_i \neq 0]
        \end{align*}
        т.е. штрафует за количество используемых параметров
\end{enumerate}

\subsection{Разновидности $L_0$ регуляризации}

\begin{enumerate}
    \item Информационный критерий Акаике 
        \begin{align*}
            \text{AIC}(\mu, X^l) = Q_{\mu}(X^l) + \frac{2\hat{\sigma}^2}{l}|J|
        \end{align*}
        где $J$ - модмножество используемых параметров, а $\hat{\sigma}^2$ - оценка дисперсии ошибки $\mathbb{D}[y_i - a(x_i)]$ 
    \item Байесовский информационный критерий
        \begin{align*}
            \text{BIC}(\mu, X^l) = \frac{l}{\hat{\sigma}^2}\left(Q_{\mu}(X^l) + \frac{\hat{\sigma}^2\ln{l}}{l}|J|\right)
        \end{align*}
    \item Оценка Вапника-Червоненкиса (VC-bound)
        \begin{align*}
            VC(\mu, X^l) = Q_{\mu}(X^l) + \sqrt{\frac{h}{l}\ln{\frac{2el}{h}} + \frac{1}{l}\ln{\frac{9}{4\eta}}}
        \end{align*}
        где $h$ - VC-размерность, которая для линейных моделей равна $|J|$, $\eta$ - уровень значимости, обычно равный $0.05$
\end{enumerate}

\subsection{Задачи}

\begin{enumerate}
    \item \textbf{Задача 1}
    
        Рассмотрим задачу линейной регрессии, вектор весов $w= [2, -1, 0, 3]$. Тренировочные данные:
        \begin{align*}
            X = \begin{bmatrix}
            1 & 2 & 3 & 4 \\
            2 & 0 & 1 & 3 \\
            3 & 1 & 0 & 2 
            \end{bmatrix} 
        \end{align*}
        \begin{align*}
            y = [10,8,7]
        \end{align*}
        Найдите:
        \begin{itemize}
        \item L2, L1, L0 штрафы
        \item BIC и AIC
        \end{itemize}
        $\tau$ везде принять за 1
        
        \textbf{Решение:} Не выписывая все выкладки, выпишем ответы:
        \begin{itemize}
            \item $L1 = \|w\|_1 = 6$
            \item $L2 = \|w\|_2^2 = 14$
            \item $L0 = \|w\|_0 = 3$
        \end{itemize}
        Для поиска AIC и BIC поднадобится дисперсия
        \begin{align*}
            \hat{\sigma}^2 = 15.0
        \end{align*}
        Тогда
        \begin{itemize}
            \item AIC = 14.12
            \item BIC = 11.42
        \end{itemize}
    \item \textbf{Задача 2}

        Докажите, что увеличение объёма выборки $l$ снижает влияние VC-размерности на обобщающую ошибку, если уровень значимости $\eta$ остаётся неизменным. Какой у этого физический смысл?

        \textbf{Решение: }
            VC-bound можно переписать следующим образом
            \begin{align*}
                VC(\mu, X^l) = Q_{\mu}(X^l) + \sqrt{\frac{h\ln{l\frac{2e}{h}} + \ln{\frac{9}{4\eta}}}{l}}
            \end{align*}
            То есть $VC(\mu, X^l) \sim Q_{\mu}(X^l) + \sqrt{\frac{\ln{l}}{l}}$
            Второ слагаемое стремится к нулю при $l \rightarrow \inf$. Это означает, что модель достаточно устойчива к переобучению при большой тренировочной выборке, поэтому внешний штраф не так уж и нужен и большую роль влияет внутренний критерий.
    \item \textbf{Задача 3}

        Объясните, почему L2-регуляризация не приводит к разреженности вектора параметров $w$, в отличие от L1-регуляризации.
\end{enumerate}
