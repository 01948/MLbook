\section{SVM-регрессия}
\subsection{Постановка задачи}
\par В задаче регрессии требуется найти функцию \( f(x) = w^T \phi(x) + b \), которая аппроксимирует целевые значения \( y \) на основе входных данных \( x \), минимизируя ошибки предсказания.
\par В SVM-регрессии вводится допустимая область погрешностей — \(\epsilon\)-окрестность. Это означает, что отклонения \( |f(x_i) - y_i| \) в пределах \(\epsilon\) считаются несущественными, и модель игнорирует их. Цель — минимизировать сложность модели, связанную с \(\|w\|\), штрафуя при этом за отклонения, выходящие за пределы \(\epsilon\).

\subsection{Прямая задача}
\par\textbf{Функция потерь.}  
Для задачи SVM-регрессии используется \(\epsilon\)-чувствительная функция потерь:
\begin{equation*}
    L_\epsilon(f(x), y) = 
    \begin{cases} 
        0, & \text{если } |f(x) - y| \leq \epsilon, \\ 
        |f(x) - y| - \epsilon, & \text{иначе.}
    \end{cases}
\end{equation*}
\noindent\textbf{Прямая постановка задачи:}
\begin{equation*}
    \min_{w, b} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n L_\epsilon(f(x_i), y_i),
\end{equation*}
где:
\begin{itemize}
    \item \(\|w\|^2\) — регуляризационный член, минимизирующий сложность модели,
    \item \(C\) — коэффициент, регулирующий баланс между штрафами за ошибки и сложностью модели,
    \item \(L_\epsilon(f(x_i), y_i)\) — штраф за выход за пределы \(\epsilon\)-окрестности.
\end{itemize}

\subsection{Преобразование задачи}
\par Для учёта отклонений выше \(\epsilon\) вводятся штрафные переменные \(\xi_i\) и \(\xi_i^*\):  
\begin{itemize}
    \item \(\xi_i\) — превышение сверху (\(y_i > f(x_i) + \epsilon\)),
    \item \(\xi_i^*\) — превышение снизу (\(y_i < f(x_i) - \epsilon\)).
\end{itemize}
\par Задача минимизации принимает вид:
\begin{equation*}
    \min_{w, b, \xi, \xi^*} \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*),
\end{equation*}
при ограничениях:
\begin{equation*}
\begin{aligned}
    y_i - (w^T \phi(x_i) + b) \leq \epsilon + \xi_i, \\
    (w^T \phi(x_i) + b) - y_i \leq \epsilon + \xi_i^*, \\
    \xi_i, \xi_i^* \geq 0.
\end{aligned}
\end{equation*}

\subsection{Метод Лагранжа}
\par Для решения задачи вводится лагранжиан, который включает:
\begin{itemize}
    \item Целевую функцию,
    \item Ограничения через множители Лагранжа (\(\alpha, \alpha^*, \eta, \eta^*\)).
\end{itemize}
\par Лагранжиан записывается как:
\begin{equation*}
\begin{aligned}
    L(w, b, \xi, \xi^*, \alpha, \alpha^*, \eta, \eta^*) &= \frac{1}{2} \|w\|^2 + C \sum_{i=1}^n (\xi_i + \xi_i^*) - \\
    &\quad - \sum_{i=1}^n \alpha_i \big[ \epsilon + \xi_i - y_i + w^T \phi(x_i) + b \big] - \\
    &\quad - \sum_{i=1}^n \alpha_i^* \big[ \epsilon + \xi_i^* + y_i - w^T \phi(x_i) - b \big] - \\
    &\quad - \sum_{i=1}^n (\eta_i \xi_i + \eta_i^* \xi_i^*).
\end{aligned}
\end{equation*}
\par Для нахождения двойственной задачи необходимо минимизировать \(L\) по \(w\), \(b\), \(\xi\), \(\xi^*\) и максимизировать по множителям Лагранжа.

\subsection{Условия оптимальности}
\begin{enumerate}
    \item Производная по \(w\):
    \begin{equation*}
        \frac{\partial L}{\partial w} = w - \sum_{i=1}^n (\alpha_i - \alpha_i^*) \phi(x_i) = 0 \implies 
        w = \sum_{i=1}^n (\alpha_i - \alpha_i^*) \phi(x_i).
    \end{equation*}
    \item Производная по \(b\):
    \begin{equation*}
        \frac{\partial L}{\partial b} = \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0.
    \end{equation*}
    \item Производные по \(\xi_i\) и \(\xi_i^*\):
    \begin{equation*}
        \alpha_i + \eta_i = C, \quad \alpha_i^* + \eta_i^* = C, \quad 0 \leq \alpha_i, \alpha_i^* \leq C.
    \end{equation*}
\end{enumerate}

\subsection{Двойственная задача}
\par Подставляя условия оптимальности в лагранжиан, исключаем \(w\), \(b\), \(\xi_i\), \(\xi_i^*\). Получаем двойственную задачу:
\begin{equation*}
    \max_{\alpha, \alpha^*} -\frac{1}{2} \sum_{i,j=1}^n (\alpha_i - \alpha_i^*)(\alpha_j - \alpha_j^*) K(x_i, x_j) 
    - \epsilon \sum_{i=1}^n (\alpha_i + \alpha_i^*) + \sum_{i=1}^n y_i (\alpha_i - \alpha_i^*),
\end{equation*}
где \(K(x_i, x_j) = \phi(x_i)^T \phi(x_j)\) — ядровая функция.
\par Ограничения:
\begin{equation*}
    \sum_{i=1}^n (\alpha_i - \alpha_i^*) = 0, \quad 0 \leq \alpha_i, \alpha_i^* \leq C.
\end{equation*}
\par Для решения двойственной задачи используется метод квадратичного программирования.

\subsection{Построение финальной модели}
\par После решения двойственной задачи оптимальные \(\alpha_i\) и \(\alpha_i^*\) определяют параметры модели:
\begin{equation*}
    f(x) = \sum_{i=1}^n (\alpha_i - \alpha_i^*) K(x_i, x) + b.
\end{equation*}
\par Смещение \(b\) вычисляется через опорные векторы — точки, где выполняется одно из условий:
\begin{equation*}
    y_i - (w^T \phi(x_i) + b) = \epsilon, \quad \text{или} \quad y_i - (w^T \phi(x_i) + b) = -\epsilon.
\end{equation*}
\par Опорные векторы (\(\alpha_i > 0\) или \(\alpha_i^* > 0\)) определяют форму модели.

\subsection{Выбор ядра}
\par Выбор ядра играет ключевую роль в качестве работы модели SVM-регрессии. Различные ядра по-разному преобразуют входные данные, что может существенно повлиять на точность предсказаний и обобщающую способность модели.
\par Выбор ядра зависит от особенностей данных, структуры зависимости и доступных вычислительных ресурсов. Экспериментальная проверка нескольких типов ядер и последующая оценка метрик качества модели — это ключевой этап в процессе выбора оптимального ядра.
\par На рисунке ниже представлена SVM-регрессия с тремя типами ядер: RBF, линейным и полиномиальным. 
\begin{figure}[h!]
    \includegraphics[width = 0.8\textwidth]{MLbook/chapters/svm/images/svm_regression_cmp_models.png}
    \centering
    \caption{Сравнение SVM-регрессия с разными типами ядер}
    \label{fig:kernel_comparison}
\end{figure}

\subsection{Задача 1}
\textbf{Условие}:
\par Рассмотрим следующий набор точек, лежащих на границе \(\epsilon\) - окрестности для SVM-регрессии с линейным ядром:
\begin{equation*}
    \{(1, 2), (2, 3), (3, 5), (4, 6)\}
\end{equation*}
\par Постройте регрессионную модель и найдите смещение \(b\), если \(w = 2\), \(\epsilon = 1\).
\par \noindent \textbf{Решение:}
\par Для нахождения смещения \(b\) необходимо использовать точки, которые лежат на границах \(\epsilon\)-окрестности. Мы знаем, что для таких точек выполняется равенство:
\begin{equation*}
    y_i - (w x_i + b) = \epsilon \quad \text{или} \quad y_i - (w x_i + b) = -\epsilon
\end{equation*}
\par Найдем для каждой точки \(b\), подставив их координаты в эти уравнения:
\begin{itemize}
\item Для точки \((1, 2)\):
\begin{equation*}
    2 - (2 \cdot 1 + b) = 1 \quad \Rightarrow \quad 2 - (2 + b) = 1 \quad \Rightarrow \quad -b = 1 \quad \Rightarrow \quad b = -1
\end{equation*}
\item Для точки \((2, 3)\):
\begin{equation*}
    3 - (2 \cdot 2 + b) = 1 \quad \Rightarrow \quad 3 - (4 + b) = 1 \quad \Rightarrow \quad -b = 2 \quad \Rightarrow \quad b = -2
\end{equation*}
\item Для точки \((3, 5)\):
\begin{equation*}
    5 - (2 \cdot 3 + b) = 1 \quad \Rightarrow \quad 5 - (6 + b) = 1 \quad \Rightarrow \quad -b = 2 \quad \Rightarrow \quad b = -2
\end{equation*}
\item Для точки \((4, 6)\):
\begin{equation*}
    6 - (2 \cdot 4 + b) = 1 \quad \Rightarrow \quad 6 - (8 + b) = 1 \quad \Rightarrow \quad -b = 3 \quad \Rightarrow \quad b = -3
\end{equation*}
\end{itemize}
\par Для вычисления окончательного значения смещения \(b\), усредняем найденные значения:
\begin{equation*}
    b_{\text{avg}} = \frac{-1 + (-2) + (-2) + (-3)}{4} = \frac{-8}{4} = -2
\end{equation*}
\par Таким образом, смещение \(b = -2\).
\par Регрессионная модель для SVM с линейным ядром имеет следующий вид:
\begin{equation*}
    f(x) = w x + b
\end{equation*}
\par Подставляем данное в условии значения \(w = 2\) и найденное значение \(b = -2\):
\begin{equation*}
    f(x) = 2x - 2
\end{equation*}
\par Это и есть наша линейная регрессионная модель.
\par \noindent \textbf{Ответ:} \(f(x) = 2x - 2\).

\subsection{Задача 2}
\textbf{Условие:}
\par У нас есть два набора данных для задачи регрессии:
\begin{itemize}
    \item Набор 1: \( \{(1, 2), (2, 3), (3, 4), (4, 5)\} \)    
    \item Набор 2: \( \{(1, 1), (2, 4), (3, 9), (4, 16)\} \)  
\end{itemize}
\par Предположим, что мы используем SVM-регрессию с различными типами ядер (линейное, полиномиальное, RBF). Определите, какое ядро будет оптимальным для каждого набора данных.
\par \noindent \textbf{Решение:}
\begin{itemize}
    \item Набор 1: данные имеют линейную зависимость, следовательно, линейное ядро будет лучшим выбором.

    \item Набор 2: данные имеют квадратичную зависимость, следовательно, оптимально будет использовать полиномиальное ядро второй степени.
\end{itemize}
\par \noindent \textbf{Ответ:} для первого набора данных оптимальным ядром будет линейное, для второго набора данных - полиномиальное второй степени.

\subsection{Задача 3}
\textbf{Условие:}
\par Дан набор данных для SVM-регрессии с линейным ядром: 
\begin{equation*}
    \{(1, 2), (2, 2.8), (3, 5.2), (4, 8)\}.
\end{equation*}
\par Параметры модели: \(w = 1.5\), \(b = 0.5\), \(\epsilon = 0.5\). 
\begin{enumerate}
    \item Определите, какие из точек набора данных находятся вне \(\epsilon\)-окрестности (требуют штрафных переменных \(\xi\) или \(\xi^*\)).
    \item Вычислите значения штрафных переменных для этих точек.
\end{enumerate}
\par \noindent \textbf{Решение:}
\begin{enumerate}
\item Определение границ \(\epsilon\)-окрестности:
   Уравнение модели SVM-регрессии с линейным ядром: 
   \begin{equation*}
    f(x) = wx + b.
   \end{equation*}
   Подсталяем данные в условии значения:
   \begin{equation*}
    f(x) = 1.5x + 0.5.
   \end{equation*}
   Границы \(\epsilon\)-окрестности:
   \begin{equation*}
    f(x) - \epsilon \leq y \leq f(x) + \epsilon.
   \end{equation*}
\item Проверка точек:
\begin{itemize}
    \item Для точки \((1, 2)\): 
    \begin{equation*}
        f(1) = 1.5 \cdot 1 + 0.5 = 2.0, \quad 2 - 0.5 \leq 2 \leq 2 + 0.5 \quad (\text{в окрестности}).
    \end{equation*}
    \item Для точки \((2, 2.8)\): 
    \begin{equation*}
        f(2) = 1.5 \cdot 2 + 0.5 = 3.5, \quad 3.5 - 0.5 \not\leq 2.8 \leq 3.5 + 0.5 \quad (\text{вне окрестности}).
    \end{equation*}
    \item Для точки \((3, 5.2)\): 
    \begin{equation*}
        f(3) = 1.5 \cdot 3 + 0.5 = 5.0, \quad 5.0 - 0.5 \leq 5.2 \leq 5.0 + 0.5 \quad (\text{в окрестности}).
    \end{equation*}
    \item Для точки \((4, 8)\): 
    \begin{equation*}
        f(4) = 1.5 \cdot 4 + 0.5 = 6.5, \quad 6.5 - 0.5 \leq 8.0 \not\leq 6.5 + 0.5 \quad (\text{вне окрестности}).
    \end{equation*}
\end{itemize}
\item Штрафные переменные:
\begin{itemize}
    \item Для точки \((2, 2.8)\):
    \begin{equation*}
        \xi_i = f(2) - y - \epsilon = 3.5 - 2.8 - 0.5 = 0.2.
    \end{equation*}
    \item Для точки \((4, 8)\):
    \begin{equation*}
        \xi_i^* = y - f(4) - \epsilon = 8 - 6.5 - 0.5 = 1.0.
    \end{equation*}
\end{itemize}
Таким образом, штрафные переменные:
\begin{equation*}
    \xi_2^* = 0.2, \quad \xi_4 = 1.0.
\end{equation*}
\end{enumerate}
\par \noindent \textbf{Ответ:} \(\xi_2^* = 0.2, \quad \xi_4 = 1.0.\)


\setcounter{secnumdepth}{0}

\section{1-norm SVM (LASSO SVM)}
\subsection*{Аппроксимация эмпирического риска с \(L_1\)-регуляризацией}
\begin{align*}
    \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \mu \sum_{j=1}^{n} |w_j| & \rightarrow \min_{w, w_0}
\end{align*}

\subsection*{Плюс: отбор признаков с параметром селективности \(\mu\)}
\begin{itemize}
    \item чем больше \(\mu\), тем меньше признаков останется
\end{itemize}

\subsection*{Минус: слишком агрессивный отбор признаков}
\begin{itemize}
    \item по мере увеличения \(\mu\) признак может быть отброшен, хотя $y$ существенно зависит от него (даже когда ещё не все шумовые признаки отброшены)
\end{itemize}

\newline
\newline

\section{Сравнение \(L_2\) и \(L_1\) регуляризации}

Зависимость весов \(w_j\) от коэффициента \(\frac{1}{\mu}\):

\begin{itemize}
    \item \(L_1\) регуляризатор: \(\mu \sum_{j} |w_j|\)
\end{itemize}

\begin{align*}
    \centering
    \includegraphics[width=0.6 \linewidth]{chapters/svm/images/L_1.png}
    \label{fig:image}    
\end{align*}

\begin{itemize}
    \item \(L_2\) регуляризатор: \(\mu \sum_{j} w_j^2\)
\end{itemize}

\begin{align*}
    \centering
    \includegraphics[width=0.6 \linewidth]{chapters/svm/images/L_2.png}
    \label{fig:image}    
\end{align*}

\section{Doubly Regularized SVM (Elastic Net SVM)}

\begin{align*}
    C \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \mu \sum_{j=1}^{n} |w_j| + \frac{1}{2} \sum_{j=1}^{n} w_j^2 & \rightarrow \min_{w, w_0}
\end{align*}

\subsection*{Плюсы:}
\begin{itemize}
    \item Параметр селективности \(\mu\) управляет отбором признаков: чем больше \(\mu\), тем меньше признаков останется
    \item Есть эффект группировки (grouping effect): значимые зависимые признаки отбираются вместе
\end{itemize}

\subsection*{Минусы:}
\begin{itemize}
    \item Шумовые признаки также группируются и могут вместе оставаться в модели
    \item Приходится подбирать два параметра регуляризации \(\mu, \tau\) (есть специальные методы, например, regularization path)
\end{itemize}

\subsection{Elastic Net Analysis}

Elastic Net менее жёстко отбирает признаки.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\linewidth]{chapters/svm/images/Elastic_Net.png}
    \caption{Зависимости весов \(w_j\) от коэффициента \(\log \frac{1}{\mu}\)}
    \label{fig:mpr}
\end{figure}

\section{Support Features Machine (SFM)}

\begin{align*}
    C \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \sum_{j=1}^{n} R_{\mu}(w_{j}) & \rightarrow \min_{w, w_0}
\end{align*}

\begin{align*}
R_{\mu}(w_j)=
    \begin{cases}
        2\mu |w_j|, & |w_j| \leq \mu\\
        \mu^2 + w_j^2, & |w_j| \geq \mu \\
    \end{cases}
\end{align*}

\subsection*{Плюсы}
\begin{itemize}
    \item Только один параметр регуляризации \(\mu\)
    \item Отбор признаков с параметром селективности \(\mu\)
    \item Эффект группировки: значимые зависимые признаки ($|w_j|$ > \(\mu\)) входят в решение совместно (как в $Elastic$ $Net$)
    \item Шумовые признаки ($|w_j|$ < \(\mu\)) не группируются и подавляются независимо друг от друга (как в $LASSO$)
\end{itemize}

\section{Relevance Features Machine (RFM)}

\begin{align*}
    C \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \sum_{j=1}^{n} \ln(w_j^2 + \frac{1}{\mu}) & \rightarrow \min_{w, w_0}
\end{align*}

\begin{align*}
    R(w) = \ln(w^2 + \frac{1}{\mu}), \quad \mu = 0.1, 1, 100
\end{align*}

\subsection*{Плюсы}
\begin{itemize}
    \item Только один параметр регуляризации \(\mu\)
    \item Отбор признаков с параметром селективности \(\mu\)
    \item Есть эффект группировки
    \item Лучше отбирает набор значимых признаков, когда они только совместно обеспечивают хорошее решение
\end{itemize}

\section{Задачи}

\subsection{Задача 1}

Качественно объяснить, почему $L_1$-регуляризатор приводит к отбору признаков

\subsection{Ответ:}

Аппроксимация эмпирического риска с \(L_1\)-регуляризацией:
\begin{align*}
    \sum_{i=1}^{\ell} \left(1 - M_i(w, w_0)\right)_+ + \mu \sum_{j=1}^{n} |w_j| & \rightarrow \min_{w, w_0}
\end{align*}

\textcolor{red}{Почему \(L_1\)-регуляризатор приводит к отбору признаков?}

Замена переменных: 
\[
u_j = \frac{1}{2} (|w_j| + w_j), \quad v_j = \frac{1}{2} (|w_j| - w_j).
\]
Тогда 
\[
w_j = u_j - v_j \quad |w_j| = u_j + v_j.
\]

\begin{align*}
    \sum_{i=1}^{\ell} \left(1 - M_i(u - v, w_0)\right)_+ + \mu \sum_{j=1}^{n} (u_j + v_j) & \rightarrow \min_{u, v}, \\
    u_j \geq 0, \quad v_j \geq 0, \quad j = 1, \ldots, n.
\end{align*}

чем больше \(\mu\), тем больше индексов \(j\) таких, что \(u_j = v_j = 0\), но тогда \(w_j = 0\), значит, \textcolor{red}{признак не учитывается.}

\subsection{Задача 2}

Привести пример нежелательного эффекта в процессе обучения, с которым поможет справиться регуляризация 
 
\subsection{Ответ:}

Регуляризация помогает в случае линейной зависимости (мультиколлинеарности) признаков:\\
Пусть построен классификатор: $a(x, w) = sign\langle w, x \rangle$ \\
Мультиколлинеарность: $\exists$  $u \in \mathbb{R}^{n}$: $\forall x \in X$ $\langle u, x \rangle = 0$ \\
Неединственность решения и рост нормы вектора весов: $\forall \gamma \in \mathbb{R}$ $a(x, w) = sign\langle w, x \rangle = sign \langle w + \gamma u, x \rangle$ \\
\\
Проявления переобучения:
\begin{itemize}
    \item слишком большие веса $|w_j|$ разных знаков
    \item неустойчивость дискриминантной функции $\langle w, x \rangle$
    \item $Q(X^{\ell}) \ll Q(X^{k})$
\end{itemize}
Способ уменьшить переобучение:\\
регуляризация $||w|| \rightarrow min$ (сокращение весов, $weight$ $decay$)

\subsection{Задача 3}

Дана задача оптимизации:
\begin{align*}
    \frac{1}{2}(wx - b)^2 + \lambda|w| & \rightarrow \min_{w},
\end{align*}
где $x,$ $b$ $\in \mathbb{R};$ $\lambda \geq 0$\\
При каких $\lambda$ данная задачи имеет решение $w_0 \neq 0?$
 
\subsection{Ответ:}

Находим правую и левую односторонние производные в нуле и рассматриваем, когда они больше и меньше 0 соответственно:

\begin{align*}
    \begin{cases}
        -xb + \lambda > 0\\
        -xb - \lambda < 0\\
        \lambda \geq 0\\
    \end{cases} \Leftrightarrow \lambda > |xb|
\end{align*}

Это условие на $\lambda,$ при котором задача имеет решение $w_0 = 0,$ поэтому нам подходит $\lambda \in [0;$ $|xb|)$.


\section{Relevance Vector Machine(RVM)}

\subsection{Идея метода релевантных векторов}

Берем за основу структуру решения как в SVM:\\
\begin{align*}
   \sum_{i=1}^{\ell} \lambda_i y_i x_i
\end{align*}

(опорным векторам $x_i$ соответствуют $\lambda_i \neq 0$) \\
\textbf{Проблема:} Какие из коэффициентов $\lambda_i$ лучше обнулить? \\
Делаем так, что регуляризатор зависит не от w, а от $\lambda_i$. \\
\begin{align*}
       \rho (\lambda) = \frac{1}{(2\pi)^{l/2} \sqrt{\alpha_1...\alpha_l}} exp(-\sum_{i=1}^{\ell} \frac{\lambda_i^2}{2\alpha_i})
\end{align*}
- т.е. $\lambda_i$ - независимые, гауссовские и с дисперсиями $\alpha_i$ \\
Будем решать задачу оптимизации с регуляризатором (логарифмируем плотность - дисперсию не считаем константой): \\
\begin{align*}
       \frac{1}{2} \sum_{i=1}^{\ell} ln\alpha_i + \frac{\lambda_i^2}{\alpha_i}
\end{align*} 
Задача оптимизации:  \\
\begin{align*}
       \sum_{i=1}^{\ell} (1 - M_i(w(\lambda), w_0))_+ + \frac{1}{2} \sum_{i=1}^{\ell} ln\alpha_i + \frac{\lambda_i^2}{\alpha_i} \rightarrow \min_{\lambda, \alpha}
\end{align*}
Если одновременно оптимизируем и $\lambda$ и $\alpha$, то при уменьшении дисперсию, $\lambda$ будет маленькая - то есть "обнуляться" и соответствующие объекты не будут опорными. 

\subsection*{Плюсы}
\begin{itemize}
    \item Опорных векторов, как правило меньше (более "разреженное" решение)
    \item Шумовые вабросы уже не входят в число опорных
    \item Не надо искать параметр регуляризации (вместо этого $\alpha_i$ оптимизируется в процессе обучения)
    \item Аналогично SVM, можно использовать ядра
\end{itemize}

\subsection*{Минусы}
\begin{itemize}
    \item Не всегда есть преимущество по качеству классификации
\end{itemize}

\subsection{Задачи по RVM}
\textbf{Задача 1:}\\
Объяснить, как метод релевантных векторов (Relevance Vector Machine, RVM) достигает разреженности в параметрах модели.  \\
\textbf{Ответ:}\\
Метод релевантных векторов (RVM) достигает разреженности через байесовскую структуру, устанавливая независимые нормальные априорные распределения с нулевым средним для весов модели, каждый с собственной гиперпараметром точности (обратная дисперсия). Конкретно, для каждого веса $w_i$ существует связанный гиперпараметр точности $\alpha_i$. 
Априорное распределение весов задается как:\\
Априорное распределение весов задается как:
\begin{align*}
       p(\mathbf{w} | \boldsymbol{\alpha}) = \prod_{i=1}^N \mathcal{N}(w_i | 0, \alpha_i^{-1})
\end{align*}

В процессе обучения RVM определяет гиперпараметры  $\boldsymbol{\alpha} = [\alpha_1, \alpha_2, \dots, \alpha_N]$, максимизируя маржинальное правдоподобие данных. Многие из этих гиперпараметров стремятся к бесконечности, что приводит к обнулению соответствующих весов $ w_i$. 

\textbf{Задача 2:}\\
Объяснить, в чем заключается основное отличие метода релевантных векторов (RVM) от метода опорных векторов (SVM), если рассматривать их с точки зрения подхода к регуляризации и использования априорных предположений. \\
\textbf{Ответ:}
\begin{itemize}
    \item Основное отличие заключается в том, что RVM использует байесовский подход, тогда как SVM — детерминированный метод. 
    \item В RVM вводятся априорные распределения на веса модели, что позволяет автоматически выполнять регуляризацию, оставляя наиболее значимые параметры.
    \item В результате RVM достигает разреженности (аналогично SVM) без использования параметра регуляризации, а лишь за счет максимизации апостериорной вероятности.

\end{itemize}


\textbf{Задача 3:}\\
Объяснить, в чем метод релевантных векторов может быть быстрее и медленнее метода опорных векторов \\
\textbf{Ответ:} \\
\textbf{Быстрее, потому что:}
 \begin{itemize}
    \item В RVM гораздо быстрее применение модели к новым точкам, потому что опорных векторов гораздо меньше 
    \item В SVM необходимо оптимизировать параметр регуляризации C (или аналогичный параметр для других ядер), что требует дополнительных затрат времени. В RVM регуляризация происходит автоматически через байесовский подход (гиперпараметры $\alpha_i$), устраняя необходимость выбора таких параметров вручную.

\end{itemize}
\textbf{Медленнее, потому что:}
\begin{itemize}
    \item Байесовский подход в RVM делает процесс обучения более ресурсоемким, чем у SVM. Это связано с итеративными расчетами для апостериорных вероятностей и гиперпараметров - обучение происходит дольше
\end{itemize}

\section{Резюме по линейным классификаторам}
\begin{itemize}
    \item SVM - лучший метод линейной классификации
    \item SVM изящно обобщается для нелинейной классификации, для линейной и нелинейной регрессии
    \item Аппрксимация пороговой функции потерь $\L (M)$ увеличивает зазор и повышает качество классификации
    \item Регуляризация устраняет мультиколлинеарность и уменьшает переобучение
    \item Негладкость функции потерь приводит к отбору объектов
    \item Негладкость регуляризатора приводит к отбору признаков
\end{itemize}
\textbf{Комментарий по последним двум пунктам:} \\
Отбор опорных объектов возник из-за того, что возникает негладкая функция потерь, аналогично (но смотрим с другой стороны)  отбор признаков так же связан с негладкостью регуляризатора. \\
Что мы можем отбирать в задаче классификации? \\
У нас есть матрица объекты - признаки, то есть либо строки, либо столбцы являются лишними.\\ Cоответственно и возникает подход: решаем задачу классификации, отфильтровав матрицу по строкам и по столбцам. \\
Фильтрация может быть реализована:
\begin{itemize}
    \item путем выбора негладких функций потерь, которые приводят к фильтрации строк (набора объектов)
    \item путем выбора негладких регуляризаторов, которые приводят к фильтрации столбцов (набора признаков)
\end{itemize}
Роль регуляризации возникает благодаря выписыванию оптимизационной задачи с ограничениями-неравенствами $\rightarrow$ требуется применять условие Каруша-Куна-Таккера (применимы только к гладким функциям) \\ 
Если функция негладкая $\rightarrow$ вводим дополнительные переменные, неотрицательные и могут обращаться в ноль $\rightarrow$ происходит отбор \\
